{"cells":[{"cell_type":"markdown","metadata":{"id":"GqWxVIxakBM7"},"source":["# Data Preprocessing\n","\n","Il dataset, prima di tutto, va opportunamente elaborato per occuparsi di eventuali standardizzazioni e feature da eliminare. Inoltre dovremo separare i dati per ottenere un training e un test set.\n","Gli step che seguiremo saranno i seguenti:\n","1. Caricare i dati\n","2. Eliminare le features ridondanti o inutili\n","3. Standardizzare i dati\n","4. Dividere il dataset in training e test set\n","5. Creazione di tensori e dataloader"]},{"cell_type":"markdown","metadata":{"id":"-Eoqv9IokKVC"},"source":["### Definizione variabili\n","\n","Per velocizzare il processo di prova di nuovi modelli, vengono dichiarate qui alcune `variabili` che verranno utilizzate successivamente nel codice."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6GeZbehNNax2"},"outputs":[],"source":["# Lunghezza percentuale del dataset di allenamento, il test è definito di conseguenza\n","len_percentage_train = 80 / 100\n","\n","classes = (\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\")"]},{"cell_type":"markdown","metadata":{"id":"IaBewKuvNax3"},"source":["### Librerie\n","\n","Allo stesso modo si inseriscono qui tutte le `librerie` usate per il progetto"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"17K-hyQxNax3"},"outputs":[],"source":["import torch\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import sys\n","import albumentations as A  # Questa libreria è servità per la data augmentation\n","from sklearn.metrics import confusion_matrix, classification_report\n","import seaborn as sn\n","IN_COLAB = 'google.colab' in sys.modules"]},{"cell_type":"markdown","metadata":{"id":"Z1bpGRBckN8e"},"source":["### Caricamento dei dati\n","\n","Tramite `pandas` e il dataset fornito in .csv, carichiamo i dati all'interno di un dataset.\n","\n","La variabile `IN_COLAB` permette al sistema di distinguere se il codice è eseguito in locale o all'interno di _Google Colab_. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":441},"executionInfo":{"elapsed":28273,"status":"ok","timestamp":1638986593770,"user":{"displayName":"Riccardo Lunardi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOLqvlm8PcUAGFVPWbdrl6nABLjES31yTkFo78zQ=s64","userId":"12776781988421852239"},"user_tz":-60},"id":"ZGgxhRzBNax4","outputId":"48f127e4-5776-456e-dfac-06dbbbcb4271"},"outputs":[],"source":["if IN_COLAB:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    fer2013 = pd.read_csv('/content/drive/My Drive//Università//Deep Learning//emotion-cnn//files//fer2013.csv')\n","else:\n","    fer2013 = pd.read_csv('files/fer2013.csv')\n","fer2013"]},{"cell_type":"markdown","metadata":{"id":"WxOMx-K-Nax5"},"source":["### Eliminazione della colonna \"Usage\"\n","\n","All'interno del dataset è presente una colonna `Usage`, che è stata utilizzata durante la challenge da cui questo dataset è stato preso. Nel nostro caso non è utile, quindi la eliminiamo, utilizzando il metodo `.drop`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":288,"status":"ok","timestamp":1638986596131,"user":{"displayName":"Riccardo Lunardi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOLqvlm8PcUAGFVPWbdrl6nABLjES31yTkFo78zQ=s64","userId":"12776781988421852239"},"user_tz":-60},"id":"mfLHzjixNax6","outputId":"0ce9041c-3854-42f0-8db6-e57a03851081"},"outputs":[],"source":["print(list(fer2013.columns))\n","fer2013 = fer2013.drop(['Usage'], axis=1)\n","print(list(fer2013.columns))"]},{"cell_type":"markdown","metadata":{"id":"LzfvJ_cUNax6"},"source":["### Da stringa di pixels a `np.array`\n","In questo passaggio elaboriamo il campo `pixel` del dataset. Di default questo è una stringa di numeri separati da uno spazio.\n","\n","Per trasformare questa stringa in un array di `numpy`:\n","1. Separiamo tutti i numeri con `.split(' ')`, che vengono subito inseriti in un array di `numpy`\n","2. L'array viene trasformato in una matrice 48x48: a questo punto i numero non verranno più considerati come stringhe, ma come `float32`\n","3. Tutti i valori vengono divisi per 255, in modo da avere solo numeri compresi da 0 e 1."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ObYFNnvnNax7"},"outputs":[],"source":["fer2013.pixels = fer2013.pixels.apply(lambda x: np.array(x.split(' ')).reshape(1, 48, 48).astype('float32')/255)"]},{"cell_type":"markdown","metadata":{"id":"DNvCPSMENax7"},"source":["### Salvataggio del dataset normalizzato\n","Tramite un oggetto `pickle`, salviamo il dataset processato. In questo modo non dovremo rieseguire i blocchi di codici precedenti, che sono computazionalmente pesanti."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V_UexwInNax7"},"outputs":[],"source":["if IN_COLAB:\n","    fer2013.to_pickle(\"/content/drive/My Drive/Università/Deep Learning/emotion-cnn/files/fer_norm.pkl\")\n","else:\n","    fer2013.to_pickle(\"files/fer_norm.pkl\")"]},{"cell_type":"markdown","metadata":{"id":"bAhMIIwiNax7"},"source":["### Caricamento del dataset normalizzato"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"executionInfo":{"elapsed":1737,"status":"ok","timestamp":1638995650207,"user":{"displayName":"Riccardo Lunardi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOLqvlm8PcUAGFVPWbdrl6nABLjES31yTkFo78zQ=s64","userId":"12776781988421852239"},"user_tz":-60},"id":"-5HHkW3iNax8","outputId":"cdfbeea8-30de-4c7b-caab-3874d1512a1b"},"outputs":[],"source":["if IN_COLAB:\n","    fer2013 = pd.read_pickle(\"/content/drive/My Drive/Università/Deep Learning/emotion-cnn/files/fer_norm.pkl\")\n","else:\n","    fer2013 = pd.read_pickle(\"files/fer_norm.pkl\")\n","\n","fer2013"]},{"cell_type":"markdown","metadata":{"id":"AHRZEl-Bka1a"},"source":["### Divisione del dataset in training e test set\n","\n","La seguente divisione produrrà due dataset:\n","1. Il training set, rapprensentatnte l'80% del dataset iniziale\n","2. Il test set set, rappresentatnte del 20% del dataset iniziale"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":263,"status":"ok","timestamp":1638995652638,"user":{"displayName":"Riccardo Lunardi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOLqvlm8PcUAGFVPWbdrl6nABLjES31yTkFo78zQ=s64","userId":"12776781988421852239"},"user_tz":-60},"id":"v-FgrDtaNax8","outputId":"f44b1a54-361c-46a3-f75a-8da024070a92"},"outputs":[],"source":["train_set, test_set = np.split(fer2013, [ int(len_percentage_train * len(fer2013)) ] )\n","test_set.reset_index(inplace=True, drop=True) # Resettiamo gli index per semplificare le operazioni successive\n","\n","print(\"Lunghezza percentuale di:\")\n","print(\"train_set: {:.0%}:\".format( len(train_set) / len(fer2013) ))\n","print(\"test_set: {:.0%}\".format( len(test_set) / len(fer2013) ))"]},{"cell_type":"markdown","metadata":{},"source":["## Data augmentation\n","Dopo svariati test, abbiamo constatato che ci fosse bisogno di aumentare il volume di dati, in quanto, come visibile nel blocco di codice successivo, il numero di dati disponibile per emozione non è lo stesso per tutte.\n","\n","Abbiamo provato sia a generare le immagini direttamente durante il training, dentro le epoche, sia prima del training, portanto tutte le classi ad avere lo stesso numero di dati. \n","\n","Nel nostro caso, il metodo più efficace si è rivelato quello di generare le immagini **prima** del training. Come livello comune abbiamo scelto quello della classe `Happy`, che contava più immagini di tutte."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Printiamo il numero di dati per ogni classe, salavando il nuero di dati più grande\n","max_elements = 0\n","\n","for i, emote in enumerate(classes):\n","    print(emote, \"-->\", len(train_set[train_set.emotion == i]))\n","\n","    if len(train_set[train_set.emotion == i]) > max_elements:\n","        max_elements = len(train_set[train_set.emotion == i])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["transform_train = A.Compose([\n","    A.HorizontalFlip(p=0.5), # Flip dell'immagine sull'asse y (immagine specchiata), con probabilità del 50% \n","    A.RandomBrightnessContrast(p=0.3), # Randomizzazione della luminosità e del contrasto dell'immagine, con probabilità del 30%\n","    A.Rotate(limit = 30) # L'immagine viene ruotata in un range che va da un angolo di -30 a 30 gradi\n","])\n","\n","def generate_data(elements_to_generate, transform_seq, emotion, images):\n","    i = 0\n","    result = []\n","    while i < elements_to_generate:\n","        for image in images:\n","            if i >= elements_to_generate:\n","                break\n","            \n","            transformed = transform_train(image=image)\n","            transformed_image = transformed[\"image\"]\n","            em_img = emotion,transformed_image\n","            result.append(em_img)\n","            i+=1\n","    \n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["new_data = []\n","for idx, emotion in enumerate(classes):\n","    train_set = pd.concat([\n","        train_set,\n","        pd.DataFrame(\n","            generate_data(max_elements - len(train_set[train_set.emotion == idx]), transform_train, idx, train_set[train_set.emotion == idx].pixels),\n","            columns=[\"emotion\", \"pixels\"]\n","        )\n","    ])\n","\n","train_set = train_set.reset_index(drop=True)"]},{"cell_type":"markdown","metadata":{},"source":["### Analisi finale del dataset\n","A questo punto la quantità di dati di train sono tutti allo stesso livello:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["freq = [0] * 7\n","for image in train_set.emotion:\n","    freq[image] += 1\n","\n","print(\"Train:\", freq)\n","\n","freq = [0] * 7\n","for image in test_set.emotion:\n","    freq[image] += 1\n","\n","print(\"Test:\", freq)"]},{"cell_type":"markdown","metadata":{"id":"r0369MBSNax9"},"source":["### Creazione tensori e dataloader\n","Partendo dagli array numpy, contenenti le immagini, dividiamo l'input e l'output per il training e il test set, che successivamente utilizziamo per creade il `TensorDataset`.\n","\n","Tramite il `TensorDataset`, creiamo i `DataLoader`: utilizziamo un batch da 32, droppando l'ultimo batch per evitare incogruenze durante il training.\n","Impostiamo anche il parametro `shuffle` a `True`: non ha importanza l'ordine con cui i dati vengono forniti al modello. È anche necessario visto che i nuovi dati generati, concatenati al dataset, sono stati inseriti in ordine di \"emozione\"."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12266,"status":"ok","timestamp":1638996373301,"user":{"displayName":"Riccardo Lunardi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOLqvlm8PcUAGFVPWbdrl6nABLjES31yTkFo78zQ=s64","userId":"12776781988421852239"},"user_tz":-60},"id":"sw5mNGLrNax9","outputId":"87b6d335-6209-4e39-f3f7-2ceaf6fdbddb"},"outputs":[],"source":["train_x, train_y = torch.tensor(train_set.pixels), torch.tensor(train_set.emotion)\n","test_x, test_y = torch.tensor(test_set.pixels), torch.tensor(test_set.emotion)\n","\n","train_dataset = torch.utils.data.TensorDataset(train_x, train_y)\n","test_dataset = torch.utils.data.TensorDataset(test_x, test_y)\n","\n","train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, drop_last=True, batch_size=32)\n","test_loader = torch.utils.data.DataLoader(test_dataset, shuffle=True, drop_last=True, batch_size=32)\n","\n","print(train_x.shape)\n","print(train_y.shape)\n","print(test_x.shape)\n","print(test_y.shape)"]},{"cell_type":"markdown","metadata":{"id":"62d9x_bKNax9"},"source":["### Mostrare le immagini\n","Definiamo la seguente funzione per visualizzare le immagini:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":248},"executionInfo":{"elapsed":243,"status":"ok","timestamp":1638995680242,"user":{"displayName":"Riccardo Lunardi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOLqvlm8PcUAGFVPWbdrl6nABLjES31yTkFo78zQ=s64","userId":"12776781988421852239"},"user_tz":-60},"id":"xQpTcUi_Nax9","outputId":"4423330d-c945-40fb-d98b-51602f8f6c50"},"outputs":[],"source":["def show(img):\n","    img = img * 255\n","    plt.axis(\"off\")\n","    plt.imshow(img, cmap = 'gray')\n","    plt.show()\n","\n","show(fer2013.pixels[4][0])"]},{"cell_type":"markdown","metadata":{"id":"farKCcO6Nax-"},"source":["# Modello con CNN"]},{"cell_type":"markdown","metadata":{"id":"p83dfp4YNax-"},"source":["### Definizione del modello"]},{"cell_type":"markdown","metadata":{},"source":["#### 3 Conv2D + 2 Linear"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Emotion_CNN(torch.nn.Module):\n","    def __init__(self):\n","        super(Emotion_CNN, self).__init__()\n","    \n","        self.num_input_conv = 1\n","        self.num_filter_conv1 = 8\n","        self.num_filter_conv2 = 16\n","        self.num_filter_conv3 = 32\n","\n","        self.cnn_layers = torch.nn.Sequential(\n","            # Primo convolution layer\n","            torch.nn.Conv2d(self.num_input_conv, self.num_filter_conv1, kernel_size=3, stride=1, padding=1),      \n","            torch.nn.BatchNorm2d(self.num_filter_conv1),\n","            torch.nn.MaxPool2d(kernel_size=3, stride=1),\n","            torch.nn.ReLU(),\n","            # Secondo 2D convolution layer\n","            torch.nn.Conv2d(self.num_filter_conv1, self.num_filter_conv2, kernel_size=3, stride=1, padding=0),\n","            torch.nn.BatchNorm2d(self.num_filter_conv2),\n","            torch.nn.MaxPool2d(kernel_size=3, stride=1),\n","            torch.nn.ReLU(),\n","            # Terzo another 2D convolution layer\n","            torch.nn.Conv2d(self.num_filter_conv2, self.num_filter_conv3, kernel_size=3, stride=1, padding=0),\n","            torch.nn.BatchNorm2d(self.num_filter_conv3),\n","            torch.nn.MaxPool2d(kernel_size=6, stride=2),\n","            torch.nn.ReLU(),\n","        )\n"," \n","        self.linear_layers = torch.nn.Sequential(\n","            torch.nn.Linear(10368, 1024),\n","            torch.nn.ReLU(),\n","            torch.nn.Linear(1024, 7)\n","        )\n"," \n","    # Funzione di forward\n","    def forward(self, x):\n","        x = self.cnn_layers(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.linear_layers(x)\n","        return x"]},{"cell_type":"markdown","metadata":{},"source":["#### 2 Conv2D + 1 Linear"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Emotion_CNN(torch.nn.Module):\n","    def __init__(self):\n","        super(Emotion_CNN, self).__init__()\n","    \n","        self.num_imput_conv = 1\n","        self.num_filter_conv1 = 16\n","        self.num_filter_conv2 = 32\n","\n","        self.cnn_layers = torch.nn.Sequential(\n","            # Primo convolution layer\n","            torch.nn.Conv2d(self.num_imput_conv, self.num_filter_conv1, kernel_size=3, stride=1, padding=1),\n","            torch.nn.ReLU(),\n","            torch.nn.BatchNorm2d(self.num_filter_conv1),\n","            torch.nn.MaxPool2d(kernel_size=3, stride=1),\n","            # Secondo 2D convolution layer\n","            torch.nn.Conv2d(self.num_filter_conv1, self.num_filter_conv2, kernel_size=3, stride=1, padding=0),\n","            torch.nn.ReLU(),\n","            torch.nn.BatchNorm2d(self.num_filter_conv2),\n","            torch.nn.MaxPool2d(kernel_size=3, stride=2),\n","            torch.nn.ReLU(),\n","        )\n"," \n","        self.linear_layers = torch.nn.Sequential(\n","            torch.nn.Linear(self.num_filter_conv2 * (21 * 21), 7),\n","            torch.nn.Sigmoid()\n","        )\n"," \n","    # Funzione di forward    \n","    def forward(self, x):\n","        x = self.cnn_layers(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.linear_layers(x)\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"Yob1_9C5Nax_"},"source":["### Istanza del modello\n","\n","Nel blocco successivo impostiamo il device per il modello, in base alle disponibilità della macchina.\n","Per l'optimizer abbiamo scelto `Adam`: il `learning rate` migliore, nel nostro caso, sembra essere 0.00001."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":299,"status":"ok","timestamp":1638996402627,"user":{"displayName":"Riccardo Lunardi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOLqvlm8PcUAGFVPWbdrl6nABLjES31yTkFo78zQ=s64","userId":"12776781988421852239"},"user_tz":-60},"id":"K1AQ25HDNax_","outputId":"8e2fb4c9-e3aa-46fb-d1a1-9e6503c719d9"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = Emotion_CNN().to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n","criterion = torch.nn.CrossEntropyLoss()\n","device"]},{"cell_type":"markdown","metadata":{"id":"llL9yDMZNax_"},"source":["### Definizione di Training e Validation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zrjPpyAcNax_"},"outputs":[],"source":["train_losses = []\n","valid_losses = []\n","\n","def Train():\n","    running_loss = .0\n","    model.train()\n","\n","    for inputs,labels in train_loader:\n","        inputs = inputs.to(device)\n","        labels = labels.to(device)\n","        optimizer.zero_grad()\n","        \n","        preds = model(inputs.float())\n","        loss = criterion(preds,labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss\n","\n","    train_loss = running_loss/len(train_loader)\n","    train_losses.append(train_loss.detach().cpu().data.numpy())\n","    \n","    print(f'train_loss {train_loss}')\n","    \n","def Valid():\n","    running_loss = .0\n","    \n","    model.eval()\n","    \n","    with torch.no_grad():\n","        for inputs, labels in test_loader:\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","            optimizer.zero_grad()\n","            preds = model(inputs.float())\n","            loss = criterion(preds,labels)\n","            running_loss += loss\n","            \n","        valid_loss = running_loss/len(test_loader)\n","        valid_losses.append(valid_loss.detach().cpu().data.numpy())\n","        print(f'valid_loss {valid_loss}')"]},{"cell_type":"markdown","metadata":{"id":"7UuT9V0KNayA"},"source":["### Allenamento e Validazione del modello"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1436788,"status":"ok","timestamp":1638997841973,"user":{"displayName":"Riccardo Lunardi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOLqvlm8PcUAGFVPWbdrl6nABLjES31yTkFo78zQ=s64","userId":"12776781988421852239"},"user_tz":-60},"id":"GnaFaUJHNayA","outputId":"30f8f0a6-7e5e-42fc-e18b-a2bae1d82b4c"},"outputs":[],"source":["epochs = 20\n","\n","for epoch in range(epochs):\n","    print('epochs {}/{}'.format(epoch+1,epochs))\n","    Train()\n","    Valid()"]},{"cell_type":"markdown","metadata":{"id":"MRz3Hj1vNayA"},"source":["### Visualizzazione della loss\n","In questo grafico confrontiamo la variazione dei valori della loss durante le epoche. In molti dei nostri test la curva associata ai valori di loss calcolati durante la validazione superava quella di quelli di train, indicando così un fenomeno di **overfitting**."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":279},"executionInfo":{"elapsed":651,"status":"ok","timestamp":1638998617769,"user":{"displayName":"Riccardo Lunardi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOLqvlm8PcUAGFVPWbdrl6nABLjES31yTkFo78zQ=s64","userId":"12776781988421852239"},"user_tz":-60},"id":"kSItNFdkNayA","outputId":"120ee0db-4958-4ce3-e4bc-808d368f70bc"},"outputs":[],"source":["loss_df = pd.DataFrame( {'Train Losses':train_losses, 'Valid Losses':valid_losses }, index=list(range(len(train_losses))) )\n","\n","loss_df=loss_df.astype(float)\n","\n","xs = loss_df.plot()\n","_ = xs.set_ylabel(\"loss\")\n","_ = xs.set_xlabel(\"epoch\")"]},{"cell_type":"markdown","metadata":{"id":"cWH2zLaMNayA"},"source":["### Salvataggio dello stato di modello\n","Per evitare di dover allenare nuovamente un certo modello, abbiamo deciso di salvare il suo stato, in modo tale da poterlo ricaricare successivamente quando necessario."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QG0MTmEXNayA"},"outputs":[],"source":["if IN_COLAB:\n","    torch.save(model.state_dict(), '/content/drive/My Drive/Università/Deep Learning/emotion-cnn/files/model_2021-12-08.pth')\n","else:\n","    torch.save(model.state_dict(), 'files/modello_ottimo_54acc.pth')"]},{"cell_type":"markdown","metadata":{"id":"a3QL7SBCNayB"},"source":["### Caricamento dello stato di modello"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Os46Ufv3NayB"},"outputs":[],"source":["# Attenzione: \"model\" deve essere della stessa classe dell'oggetto che si aveva salvato\n","model.load_state_dict(torch.load('files/modello_ottimo.pth', map_location=torch.device('cuda')))"]},{"cell_type":"markdown","metadata":{"id":"KLLSrW2-NayB"},"source":["# Predizione e valutazione del modello"]},{"cell_type":"markdown","metadata":{"id":"BCgcDIsINayB"},"source":["### Valutazione del modello\n","Verifichiamo l'accuratezza del modello generale e per classe tramite il successivo blocco di codice.\n","\n","Salviamo anche i risultati del modello e le label perché ci serviranno per il calcolo della matrice di confusione."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3014,"status":"ok","timestamp":1639001871358,"user":{"displayName":"Riccardo Lunardi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOLqvlm8PcUAGFVPWbdrl6nABLjES31yTkFo78zQ=s64","userId":"12776781988421852239"},"user_tz":-60},"id":"hsYtVCK4HOHN","outputId":"068ced82-f50e-4d15-d9d0-30daf74753d0"},"outputs":[],"source":["# Variabili per il calcolo dell'accuratezza generale\n","correct = 0\n","total = 0\n","\n","# Liste necessarie per il calcolo della matrice di confusione\n","y = []\n","y_pred = []\n","\n","# Calcolo dell'accuratezza del modello (necessario per dividere per classe)\n","class_correct = list(0. for _ in range(7))\n","class_total = list(0. for _ in range(7))\n","\n","with torch.no_grad():\n","    for data in test_loader:\n","        images, labels = data \n","        images, labels = images.cuda(), labels.cuda()\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs, 1) # Supponiamo che il valore più altro sia l'emozione trovata\n","\n","        y.extend(labels.squeeze().detach().cpu().numpy())\n","        y_pred.extend(predicted.squeeze().detach().cpu().numpy())\n","\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","        c = (predicted == labels).squeeze()\n","        for i in range(8):\n","            label = labels[i]\n","            class_correct[label] += c[i].item()\n","            class_total[label] += 1\n","\n","\n","print('Accuratezza del modello: %d %%' % (100 * correct / total))\n","\n","for i in range(7):\n","    print('Accuratezza %5s : %2d %%' % (classes[i], 100 * class_correct[i] / class_total[i]))"]},{"cell_type":"markdown","metadata":{},"source":["### Matrice di confusione\n","La matrice di confusione viene calcolata tramite gli output del modello e le label reali tramite la funzione `sklearn.metrics.confusion_matrix`.\n","Per la visualizzazione utilizziamo a libreria `seaborn` e dividiamo ogni riga per il suo totale, in modo da ottenere i valori percentuali."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["cf_matrix = confusion_matrix(y, y_pred)\n","df_cm = pd.DataFrame(cf_matrix/np.sum(cf_matrix, axis=1), index = [i for i in classes], columns = [i for i in classes])\n","plt.figure(figsize = (12,7))\n","sn.heatmap(df_cm, annot=True)"]},{"cell_type":"markdown","metadata":{},"source":["### Altre metriche di valutazione\n","`classification_report` di `sklearn` permette di ottenere un riassunto, per classe, delle metriche di _precision_, _recall_, _f1-score_ e _accuratezza_.\n","Le aggregazioni di questi valori vengono effettuate tramite media artimetica e media pesata.\n","\n","La _precision_ è il rapporto tra le volte in cui una certa classe è stata riconosciuta correttamente e tutte le volte che è stata predetta.\n","\n","La _recall_ è il rapporto tra il numero di volte in cui una certa label è stata predetta correttamente e il numero totale di label di quel tipo. \n","\n","L'_f1-score_ è la media armonica tra la precision e la recall. Viene utilizzata proprio la media armonica perché viene ritenuta appropiata per fare medie tra tassi."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(classification_report(y, y_pred, digits=7))\n","metrics = classification_report(y, y_pred, digits=7, output_dict=True)\n","\n","print(\"Accuratezza generale del modello:\", f\"{round(metrics['accuracy'], 3)}%\")\n","print(\"Precisione media del modello:\", f\"{round(metrics['weighted avg']['precision'], 3)}%\")\n","print(\"Recall media del modello:\", f\"{round(metrics['weighted avg']['recall'], 3)}%\")\n","print(\"F1-Score medio del modello:\", f\"{round(metrics['weighted avg']['f1-score'], 3)}%\")"]},{"cell_type":"markdown","metadata":{},"source":["## Visualizzazione dei filtri\n","\n","I blocchi di codice sottostanti mirano alla visualizzazione dei vari kernel che vanno a crearsi durante il training del modello.\n","\n","Salviamo in una lista tutti i layer convoluzionali, da cui estrarremmo i kernel e a cui passeremo le immagini per vedere come vengono trasformate."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_weights = [] \n","conv_layers = []\n","\n","model_children = list(model.children()) # Con children otteniamo tutti i vari layer del modello\n","counter = 0 \n","# append all the conv layers and their respective weights to the list\n","for sequence in model_children:\n","    if type(sequence) == torch.nn.Sequential: \n","        for layer in sequence: # Iteriamo sui layer all'interno del sequenziale\n","            if type(layer) == torch.nn.Conv2d:\n","                counter += 1\n","                model_weights.append(layer.weight)\n","                conv_layers.append(layer)\n","\n","print(f\"Layer convoluzionali: {counter}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure(figsize=(20, 17))\n","for weight in model_weights:\n","    for i, filter in enumerate(weight):\n","        plt.subplot(8, 8, i+1)\n","        plt.imshow(filter[0, :, :].cpu().detach(), cmap='gray')\n","        plt.axis('off')\n","        plt.savefig(f'imgs_filter/48/{i}_cnn.png')\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["L'immagine che osserveremo attraverso i vari filtri è la seguente:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["show(test_set.pixels[0][0])"]},{"cell_type":"markdown","metadata":{},"source":["Ad ogni layer convoluzionale della rete passiamo l'immagine elaborata dal layer precedente, salvando il risultato in una lista"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Primo passo (primo layer)\n","outputs = [conv_layers[0](torch.tensor(test_set.pixels)[:1].cuda())]\n","for i in range(1, len(conv_layers)):\n","    # Utilizza il risultato precedente per il nuovo layer\n","    outputs.append(conv_layers[i](outputs[-1]))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for num_layer in range(len(outputs)):\n","    plt.figure(figsize=(48, 48))\n","    layer_viz = outputs[num_layer][0, :, :, :]\n","    layer_viz = layer_viz.data\n","    print(layer_viz.size())\n","    for i, filter in enumerate(layer_viz):\n","        plt.subplot(8, 8, i + 1)\n","        plt.imshow(filter.cpu(), cmap='gray') # Passiamo il risultato in cpu() per permettere la visualizzazione\n","        plt.axis(\"off\")\n","\n","    print(f\"Salvataggio delle immagini del layer {num_layer}...\")\n","    plt.savefig(f\"./imgs_filter/48/layer_{num_layer}.png\")\n","    plt.close()"]},{"cell_type":"markdown","metadata":{},"source":["# Bibiliografia\n","\n","[Build an Image Classification Model using Convolutional Neural Networks in PyTorch](https://www.analyticsvidhya.com/blog/2019/10/building-image-classification-models-cnn-pytorch/)\n","\n","[Convolutional Neural Networks for Facial Expression Recognition, Shima Alizadeh and Azar Fazel, 2017](https://arxiv.org/abs/1704.06756)\n","\n","[PyTorch's NN](https://pytorch.org/docs/stable/nn.html)\n","\n","[Precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall)\n","\n","[F-Score](https://en.wikipedia.org/wiki/F-score)\n","\n","[Visualizing Filters and Feature Maps in Convolutional Neural Networks using PyTorch](https://debuggercafe.com/visualizing-filters-and-feature-maps-in-convolutional-neural-networks-using-pytorch/)\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"emotion_cnn_dabrosca_lunardi.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}},"nbformat":4,"nbformat_minor":0}
