{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"05 - Embeddings and Sentiment with Neural Networks.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"widgets":{"application/vnd.jupyter.widget-state+json":{"1bb85751a025400ca962516dfd90bbbb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_697cfec1a51c43a18b821f6d60853adc","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f86cdc037ebd46408329e677c6df4033","IPY_MODEL_e39d3ee2025b4cda81ebc9d66ac9ae71","IPY_MODEL_219e62c9379244aa9e90ffd7bf6f930f"]}},"697cfec1a51c43a18b821f6d60853adc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f86cdc037ebd46408329e677c6df4033":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ce654102bb1044ccbea197694899a359","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2d21b532f3394c39aa137cc24e396479"}},"e39d3ee2025b4cda81ebc9d66ac9ae71":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_e9dc51a6df6e452598c893daa18cb6c5","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":24998,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":24998,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3396de412b4b4c3ab584c9ab7db508f5"}},"219e62c9379244aa9e90ffd7bf6f930f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_671edb6da7b4476a860822ed2cda1aa0","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 24998/24998 [01:05&lt;00:00, 370.70it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0b31638d3380416ab6809eac11202b0b"}},"ce654102bb1044ccbea197694899a359":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2d21b532f3394c39aa137cc24e396479":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e9dc51a6df6e452598c893daa18cb6c5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"3396de412b4b4c3ab584c9ab7db508f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"671edb6da7b4476a860822ed2cda1aa0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0b31638d3380416ab6809eac11202b0b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"acb252a5e2dd445e9c37e4514b7a73a2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_1ae7dcd628644f0ba24115c9a0a676f9","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_2e1d3276283147c383a27c7bd73077b0","IPY_MODEL_c06f37b0d7e2427e820a2e9592592340","IPY_MODEL_5c052faa436d4e37a14719cf84e8138b"]}},"1ae7dcd628644f0ba24115c9a0a676f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2e1d3276283147c383a27c7bd73077b0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_cd7b636fb9c74b449fff7a14128e9842","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_aaec0f72d6de4de4a74a736e1b650def"}},"c06f37b0d7e2427e820a2e9592592340":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c1e9ab26fae3427d81c2b361a55e62c8","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":25002,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":25002,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a24213866b2c4bbb9d12a9cd54b56f19"}},"5c052faa436d4e37a14719cf84e8138b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_058bf568db4e47eea0fe10548a660af3","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 25002/25002 [01:05&lt;00:00, 371.68it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4e30f51b5c824df4b1a98ddd271dcc43"}},"cd7b636fb9c74b449fff7a14128e9842":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"aaec0f72d6de4de4a74a736e1b650def":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c1e9ab26fae3427d81c2b361a55e62c8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"a24213866b2c4bbb9d12a9cd54b56f19":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"058bf568db4e47eea0fe10548a660af3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"4e30f51b5c824df4b1a98ddd271dcc43":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"140b2f9da23d41fb8952f76b34b94e59":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_69929bd823dc4dd7934c58f179e5d390","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_22cb2a0c7631410399dbfea808a881da","IPY_MODEL_8b458b530c424480983feed25cebb679","IPY_MODEL_2b1c0d313f904507825a7d011a551168"]}},"69929bd823dc4dd7934c58f179e5d390":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"22cb2a0c7631410399dbfea808a881da":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5a8e78a3e7b143e9b171a86c212cd824","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_82adcb67c5784587beed7aad71db093e"}},"8b458b530c424480983feed25cebb679":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_411802e10dc7404691f6584d163b1d13","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":400000,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":400000,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_16befc156ae04e12a4edac5072c1642b"}},"2b1c0d313f904507825a7d011a551168":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_df79680830a342af9131279454a6760e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 400000/400000 [01:35&lt;00:00, 4473.43it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8893ddc56c134e4eaa029973de13c9dd"}},"5a8e78a3e7b143e9b171a86c212cd824":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"82adcb67c5784587beed7aad71db093e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"411802e10dc7404691f6584d163b1d13":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"16befc156ae04e12a4edac5072c1642b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"df79680830a342af9131279454a6760e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"8893ddc56c134e4eaa029973de13c9dd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"616c38f9808849d6bd6762e86035436c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d0353c55743e42d4a2d5489a678e1c45","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_c6b65bf5aacb4a2ea66e0f38200e5a27","IPY_MODEL_6a72982c75a7468885f4c516267dea76","IPY_MODEL_dde81648134845789341c724844260f3"]}},"d0353c55743e42d4a2d5489a678e1c45":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c6b65bf5aacb4a2ea66e0f38200e5a27":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a418e1604d42403d9df12be4bafeebcb","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_fbfe9d152d3a4effa38def0b487ba898"}},"6a72982c75a7468885f4c516267dea76":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_77dca599b2df4e31a908d0dae6d0d475","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":80727,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":80727,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7ebcbe9d631243b99fe77bb90c0d0ca6"}},"dde81648134845789341c724844260f3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_926a5150002641bbb28ee235ed17469b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 80727/80727 [00:00&lt;00:00, 92091.83it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_35a02300873049a8b13906d9df6b1f22"}},"a418e1604d42403d9df12be4bafeebcb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"fbfe9d152d3a4effa38def0b487ba898":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"77dca599b2df4e31a908d0dae6d0d475":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"7ebcbe9d631243b99fe77bb90c0d0ca6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"926a5150002641bbb28ee235ed17469b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"35a02300873049a8b13906d9df6b1f22":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"tkyMVHnR4-Qv"},"source":["# Sentiment with Deep Neural Networks and Pretrained Embeddings\n","\n","\n","In this assignment, you will explore **sentiment** analysis using deep neural networks and **pretrained word embeddings**. \\\\\n","We will use a dataset of movie reviews from the IMDb (Internet Movie Database), which contains the text of the reviews and a binary label for their sentiment.\n","\n","These are main the steps:\n","\n","- [**1)  Loading the data**](#1.1)\n","  (load the csv files, extract inputs and outputs, clean and tokenize the text)\n","- [**2) Building the Vocabulary**](#2)\n","  (enumerate all tokens in the train and create a Vocabulary)\n","- [**3) Numericalizing and Padding the texts**](#3)\n","  (from texts of different lengths to equal-sized lists of integers)\n","- [**4)  Preparing the Embeddings**](#4)\n","  (download pretrained embeddings, arrange them to match the order of the words in the Vocabulary)\n","- [**5)  Setup the Model**](#5)\n","  (define the Model, setup the data loaders)\n","- [**6)  Training and Testing**](#6)\n","  (define the training / testing functions, execute training / testing)\n"]},{"cell_type":"markdown","metadata":{"id":"GA-ZU8hHGHHl"},"source":["The following is a rough schema of the Model we are going to use. It is made of:\n","- an Embedding Layer (pretrained GloVe embeddings) \\\\\n","- an averaging operation (to compute the mean embedding of the sentence) \\\\\n","- a Linear Layer (which projects the \"sentence embedding\" to the binary output space) \\\\\n","\n","\\\\\n","$$\n","\\small{(\\textit{batch_size} \\times \\textit{max_seq_len})}\\\\\n","\\boxed{\\ \\small{\\textbf{Embedding Layer}}\\ } \\\\\n","\\small{(\\textit{batch_size} \\times \\textit{embedding_size} \\times \\textit{max_seq_len})}\\\\\n","\\boxed{\\ \\small{\\textbf{Mean}}\\ } \\\\\n","\\small{(\\textit{batch_size} \\times \\textit{embedding_size})}\\\\\n","\\boxed{\\ \\small{\\textbf{Linear Layer}}\\ } \\\\\n","\\small{(\\textit{batch_size} \\times \\textit{2})}\\\\\n","$$\n"]},{"cell_type":"markdown","metadata":{"id":"mg6hJuYY4-RS"},"source":["<a name=\"1\"></a>\n","# 1)  Importing the data"]},{"cell_type":"markdown","metadata":{"id":"pC67qaE_30Zc"},"source":["The next cell will download the GloVe embeddings. It will take about 20 minutes, but it will run in the background, so you can keep using the other cells in the meantime. \\\\\n","Once the download is finished it will create a file named \"DONE\" (you can open the \"File\" section on the left to check whether the download is DONE). When you'll need the embeddings another cell will check if the download has finished."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rkQMwhFV0UpS","executionInfo":{"status":"ok","timestamp":1638025664461,"user_tz":-60,"elapsed":351,"user":{"displayName":"Riccardo Lunardi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOLqvlm8PcUAGFVPWbdrl6nABLjES31yTkFo78zQ=s64","userId":"12776781988421852239"}},"outputId":"34a4adfd-c7f2-4b71-ddc1-4d181248cb76"},"source":["import os\n","import threading\n","class Downloader(object):\n","    def __init__(self):\n","        pass\n","    def start(self):\n","        if not os.path.exists(\"glove.6B.300d.txt\"):\n","            print(\"Dowloading embeddings\")\n","            ! wget -O glove.6B.300d.txt http://ailab.uniud.it/wp-content/uploads/2019/05/glove.6B.300d.txt 2> progress.txt\n","            with open(\"DONE\", \"w\"):\n","                pass\n","        else:\n","            print(\"Embeddings already downloaded!\")\n","downloader = Downloader()\n","t = threading.Thread(target=downloader.start)\n","t.start()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dowloading embeddings\n"]}]},{"cell_type":"markdown","metadata":{"id":"2NA0IfUAfG2m"},"source":["In this first part you will\n","load two csv datasets using pandas (train and test data), \n","extract the inputs and the outputs from the datasets and\n","tokenize the sentences. \\\\\n","Let's start by downloading the two datasets."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WtSEP_fEphFs","executionInfo":{"status":"ok","timestamp":1638025813509,"user_tz":-60,"elapsed":148728,"user":{"displayName":"Riccardo Lunardi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOLqvlm8PcUAGFVPWbdrl6nABLjES31yTkFo78zQ=s64","userId":"12776781988421852239"}},"outputId":"c388181b-8212-48ff-c514-12987cfa7abf"},"source":["import os\n","\n","if not os.path.exists(\"Train_Movie_Data.csv\"):\n","    print(\"Downloading Train set\")\n","    ! wget -O Train_Movie_Data.csv http://ailab.uniud.it/wp-content/uploads/2019/05/Train_Movie_Data.csv\n","else:\n","    print(\"Train set already downloaded!\")\n","\n","if not os.path.exists(\"Test_Movie_Data.csv\"):\n","    print(\"Downloading Test set\")\n","    ! wget -O Test_Movie_Data.csv http://ailab.uniud.it/wp-content/uploads/2019/05/Test_Movie_Data.csv\n","else:\n","    print(\"Test set already downloaded!\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading Train set\n","--2021-11-27 15:07:43--  http://ailab.uniud.it/wp-content/uploads/2019/05/Train_Movie_Data.csv\n","Resolving ailab.uniud.it (ailab.uniud.it)... 158.110.145.61\n","Connecting to ailab.uniud.it (ailab.uniud.it)|158.110.145.61|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 32873269 (31M) [text/csv]\n","Saving to: ‘Train_Movie_Data.csv’\n","\n","Train_Movie_Data.cs 100%[===================>]  31.35M   414KB/s    in 72s     \n","\n","2021-11-27 15:08:56 (445 KB/s) - ‘Train_Movie_Data.csv’ saved [32873269/32873269]\n","\n","Downloading Test set\n","--2021-11-27 15:08:56--  http://ailab.uniud.it/wp-content/uploads/2019/05/Test_Movie_Data.csv\n","Resolving ailab.uniud.it (ailab.uniud.it)... 158.110.145.61\n","Connecting to ailab.uniud.it (ailab.uniud.it)|158.110.145.61|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 32989056 (31M) [text/csv]\n","Saving to: ‘Test_Movie_Data.csv’\n","\n","Test_Movie_Data.csv 100%[===================>]  31.46M   383KB/s    in 74s     \n","\n","2021-11-27 15:10:12 (436 KB/s) - ‘Test_Movie_Data.csv’ saved [32989056/32989056]\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"rn4DFHyUWIqH"},"source":["We will use a dataset of movie reviews from the IMDb (Internet Movie Database). This dataset contains the **text of the reviews**, together with a **label** that indicates whether a review is **\"positive\" or \"negative\"**. \\\\\n","Let's load the Train dataset and visualize it using `pandas`!"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pM0u2m4kTKHu","executionInfo":{"status":"ok","timestamp":1638025813954,"user_tz":-60,"elapsed":451,"user":{"displayName":"Riccardo Lunardi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOLqvlm8PcUAGFVPWbdrl6nABLjES31yTkFo78zQ=s64","userId":"12776781988421852239"}},"outputId":"9a95d7f9-092d-40c6-b8ca-c578c3577e5b"},"source":["import pandas as pd\n","\n","train = pd.read_csv('Train_Movie_Data.csv')\n","print(\"First Elements of train set:\\n {}\".format(train.head(20)))\n","print()\n","print(\"Number of Train samples:\\n\", train.shape[0])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["First Elements of train set:\n","                                                review  sentiment\n","0   In 1974, the teenager Martha Moxley (Maggie Gr...          1\n","1   OK... so... I really like Kris Kristofferson a...          0\n","2   ***SPOILER*** Do not read this, if you think a...          0\n","3   hi for all the people who have seen this wonde...          1\n","4   I recently bought the DVD, forgetting just how...          0\n","5   Leave it to Braik to put on a good show. Final...          1\n","6   Nathan Detroit (Frank Sinatra) is the manager ...          1\n","7   To understand \"Crash Course\" in the right cont...          1\n","8   I've been impressed with Chavez's stance again...          1\n","9   This movie is directed by Renny Harlin the fin...          1\n","10  I once lived in the u.p and let me tell you wh...          0\n","11  Hidden Frontier is notable for being the longe...          1\n","12  It's a while ago, that I have seen Sleuth (197...          0\n","13  What is it about the French? First, they (appa...          0\n","14  This very strange movie is unlike anything mad...          1\n","15  I saw this movie on the strength of the single...          0\n","16  There are some great philosophical questions. ...          0\n","17  I was cast as the Surfer Dude in the beach sce...          1\n","18  I had high hopes for this one until they chang...          0\n","19  Set in and near a poor working class town in t...          1\n","\n","Number of Train samples:\n"," 24998\n"]}]},{"cell_type":"markdown","metadata":{"id":"SrEsbDj2TvTj"},"source":["Let's save our inputs (reviews) and outputs (sentiments) in two arrays: `X_train` and `y_train`."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bMH1_hL9UEMh","executionInfo":{"status":"ok","timestamp":1638025813955,"user_tz":-60,"elapsed":8,"user":{"displayName":"Riccardo Lunardi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOLqvlm8PcUAGFVPWbdrl6nABLjES31yTkFo78zQ=s64","userId":"12776781988421852239"}},"outputId":"835d5f88-4d34-433f-f29a-d131c96ae732"},"source":["X_train = train['review'].values\n","y_train = train['sentiment'].values\n","\n","print(\"X_train:\\n\", X_train)\n","print(\"\\ny_train:\\n\", y_train)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["X_train:\n"," ['In 1974, the teenager Martha Moxley (Maggie Grace) moves to the high-class area of Belle Haven, Greenwich, Connecticut. On the Mischief Night, eve of Halloween, she was murdered in the backyard of her house and her murder remained unsolved. Twenty-two years later, the writer Mark Fuhrman (Christopher Meloni), who is a former LA detective that has fallen in disgrace for perjury in O.J. Simpson trial and moved to Idaho, decides to investigate the case with his partner Stephen Weeks (Andrew Mitchell) with the purpose of writing a book. The locals squirm and do not welcome them, but with the support of the retired detective Steve Carroll (Robert Forster) that was in charge of the investigation in the 70\\'s, they discover the criminal and a net of power and money to cover the murder.<br /><br />\"Murder in Greenwich\" is a good TV movie, with the true story of a murder of a fifteen years old girl that was committed by a wealthy teenager whose mother was a Kennedy. The powerful and rich family used their influence to cover the murder for more than twenty years. However, a snoopy detective and convicted perjurer in disgrace was able to disclose how the hideous crime was committed. The screenplay shows the investigation of Mark and the last days of Martha in parallel, but there is a lack of the emotion in the dramatization. My vote is seven.<br /><br />Title (Brazil): Not Available'\n"," \"OK... so... I really like Kris Kristofferson and his usual easy going delivery of lines in his movies. Age has helped him with his soft spoken low energy style and he will steal a scene effortlessly. But, Disappearance is his misstep. Holy Moly, this was a bad movie! <br /><br />I must give kudos to the cinematography and and the actors, including Kris, for trying their darndest to make sense from this goofy, confusing story! None of it made sense and Kris probably didn't understand it either and he was just going through the motions hoping someone would come up to him and tell him what it was all about! <br /><br />I don't care that everyone on this movie was doing out of love for the project, or some such nonsense... I've seen low budget movies that had a plot for goodness sake! This had none, zilcho, nada, zippo, empty of reason... a complete waste of good talent, scenery and celluloid! <br /><br />I rented this piece of garbage for a buck, and I want my money back! I want my 2 hours back I invested on this Grade F waste of my time! Don't watch this movie, or waste 1 minute of your valuable time while passing through a room where it's playing or even open up the case that is holding the DVD! Believe me, you'll thank me for the advice!\"\n"," '***SPOILER*** Do not read this, if you think about watching that movie, although it would be a waste of time. (By the way: The plot is so predictable that it does not make any difference if you read this or not anyway)<br /><br />If you are wondering whether to see \"Coyote Ugly\" or not: don\\'t! It\\'s not worth either the money for the ticket or the VHS / DVD. A typical \"Chick-Feel-Good-Flick\", one could say. The plot itself is as shallow as it can be, a ridiculous and uncritical version of the American Dream. The young good-looking girl from a small town becoming a big success in New York. The few desperate attempts of giving the movie any depth fail, such as the \"tragic\" accident of the father, the \"difficulties\" of Violet\\'s relationship with her boyfriend, and so on. McNally (Director) tries to arouse the audience\\'s pity and sadness put does not have any chance to succeed in this attempt due to the bad script and the shallow acting. Especially Piper Perabo completely fails in convincing one of \"Jersey\\'s\" fear of singing in front of an audience. The only good (and quite funny thing) about \"Coyote Ugly\" is John Goodman, who represents the small ray of hope of this movie.<br /><br />I was very astonished, that Jerry Bruckheimer produced this movie. First \"Gone In 60 Seconds\" and now this... what happened to great movies like \"The Rock\" and \"Con Air\"? THAT was true Bruckheimer stuff.<br /><br />If you are looking for a superficial movie with good looking women just to have a relaxed evening, you should better go and see \"Charlie\\'s Angels\" (it\\'s much more funny, entertaining and self-ironic) instead of this flick.<br /><br />Two thumbs down (3 out of 10).'\n"," ...\n"," 'This early Anime movie was a rather good film that I caught once on the Science Fiction channel when Anime was actually popular here in America and not the ratings disaster that adult swim claims it is on the cartoon network. I quite frankly think it has less to do with it being less popular and more with the fact people would rather now buy dvds are watch the episodes uncut on the internet. This film though probably did not have all that many cuts and the voice work was okay for a dubbed movie, though I would rather watch the original Japanese version. Americans tend to use some rather annoying voices for children in anything dubbed. This film features a young boy who boards a train called the Galaxy Express in the hopes that he can make it to a planet that has the technology to turn him into a robot. He wishes to become a robot to avenge his mother, who was brutally murdered at the hands of a robot who hunts humans for fun. During the course of his adventures he becomes friends with the various workers aboard the train as well as a woman that resembles his deceased mother, a beautiful woman named Matel, who as with most woman in Anime movies has a secret that could either be really good for our young hero, or really bad. He goes from planet to planet too as the train makes various stops and he runs into a space pirate named Captain Harlock who apparently starred in his own animated cartoon series, so basically the Galaxy Express takes place in that universe. All in all a very good ride with a rather strange and unexpected ending. There would be a sequel to this one, but it was not quite as good as this one, however the ending was a bit more final than it was here.'\n"," 'A comedy that worked surprisingly well was the little British effort \"The Divorce Of Lady X (1938)\" . It marks the first pairing of Laurence Olivier and Merle Oberon, before that little film about uncontrollable passion on the 19th century English moors. And while Olivier and Oberon are not particularly well-suited to screwball comedy, it all flows along nicely. Oberon is Leslie, a young woman who ends up in priggish divorce lawyer Logan\\'s (Olivier) hotel suite by way of a nasty English fog preventing travel. She does everything possible to irritate him--but, in the crazy way films go, he falls for her. And she falls for him. But a serious case of mistaken identity occurs when Oberon\\'s \"Lady X\" (that\\'s all she leaves Oliver in a note) is thought by Olivier to be a married woman. To make matters worse, and more amusing, Lord Mere (Ralph Richardson) goes to Olivier wanting a divorce from his wife whom dear Larry thinks must be Oberon! There is some nice battle-of-the-sexes dialogue, and fun exploration of sexual politics. You can see that Olivier is not too confident with the comedy, but in true Olivier he\\'s a consummate professional, and delivers. And he handles the screwball twists and turns, maybe not with ease, but with gusto. Oberon was no great shakes as an actress, but she was usually competent enough, and despite their reputed off-screen dislike of her, worked well with Olivier. This was filmed in early Technicolour that looks very primitive today (everyone looks even whiter than Michael Jackson), but perhaps the print needs cleaning up.'\n"," \"I want to say that I went to this movie with my expectations way too high. I thought it was going to be funny because it's the sequel to Bruce Almighty which was really funny and it stars Steve Carell who is an excellent comedic actor but boy, did it sucked.<br /><br />The movie is advertised as a sequel but it really has nothing to do with the original since the only people reprising their roles are Morgan Freeman and Steve Carell but Steve's character is completely different, he is no longer the jerk he was in the first one here he is a nice guy. The story is different and the actors are different and it's not funny.<br /><br />All the actors involved(Steve Carell, Morgan Freeman, Wanda Sykes, John Goodman, Ed Helms and even Jon Stewart in a very crappy cameo) have talent but none of them seems to use it and it looks that there in the movie just for the money.<br /><br />Now the plot is obviously shaped after Noah's story but there are so many wrong things with it, I don't know where to start. I guess the big problem is that in the everyone around Evan thinks that he is crazy despite all the things that are happening to him, he grows a huge white beard in two days, he gets help from animals from all around the world, he builds a giant arc in a few weeks, in real life people wouldn't be mocking these guy after that, they would be saying he is the new Noah.<br /><br />Also the special effects are good but what the hell is the greatest movie flood ever filmed doing in Evan Almighty? Did they really had to waste such good special effects as filler for this crappy movie.<br /><br />Jim Carrey seems to be a smart guy since he has stayed away of three of the worst sequels ever made, Son of the Mask, Dumb and Dumbered and now Evan Almighty.<br /><br />This was a giant disappointment and Tom Shyadac should be ashamed of himself.\"]\n","\n","y_train:\n"," [1 0 0 ... 1 1 0]\n"]}]},{"cell_type":"markdown","metadata":{"id":"676J16C1UmsT"},"source":["Okay, let's repeat the same process for the Test data: load them and save only the needed data in `X_test` and `y_test`.\n","Check how many samples there are in the Test dataset."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z73GZR4WS3hq","executionInfo":{"status":"ok","timestamp":1638025814386,"user_tz":-60,"elapsed":434,"user":{"displayName":"Riccardo Lunardi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOLqvlm8PcUAGFVPWbdrl6nABLjES31yTkFo78zQ=s64","userId":"12776781988421852239"}},"outputId":"4b77a219-4f2f-4f30-cdb7-5fac9bb7f050"},"source":["test = pd.read_csv(\"Test_Movie_Data.csv\")  # BLANK\n","print(\"First Elements of test set:\\n {}\".format(test.head(3)))\n","print()\n","print(\"Number of Test samples:\\n\", test.shape[0])  # BLANK\n","X_test = test[\"review\"].values # BLANK\n","y_test = test[\"sentiment\"].values   # BLANK\n","\n","print()\n","print(\"X_test:\\n\", X_test)\n","print(\"\\ny_test:\\n\", y_test)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["First Elements of test set:\n","                                               review  sentiment\n","0  I have seen several comments here about Brando...          1\n","1  I liked this film very much. The story jumps b...          1\n","2  There's a part of me that would like to give t...          0\n","\n","Number of Test samples:\n"," 25002\n","\n","X_test:\n"," ['I have seen several comments here about Brando using a Southern accent, some of which felt it was a mistake. When this movie was made, racism and discrimination were very strong in the South. The Jim Crow laws were still in effect. Civil Rights was in it\\'s infancy. Could this have possibly been a subtle social commentary, a Southern man in love with a woman of another race? The same way MASH was a subtle criticism of the Viet Nam war? Any thoughts?<br /><br />Another comment was made about Myoshi Umeki appearing \"cold\". Anyone who has been in Japan would understand. The Japanese people, at least in my experience, did not tend to show emotion in front of strangers. There were strict social rules, especially for men meeting single women. Americans in Japan were totally foreign to this culture, and the blunt attempts to meet women were shocking to the ladies. One trait of the Japanese was to smile when embarrassed or uncomfortable, which many American servicemen took as a sign that their advances were welcomed. Also remember that at the time represented in the movie, Japan had just been defeated, and the occupying forces were treated with reluctant acceptance. I think Myoshi Umeki gave a very credible performance of what her situation would have been. Watching her interaction with the American actors brought back several memories of my own experiences in the country. I was able to meet a pair of lovely young ladies who, after I convinced them I was not the typical American male, taught me their language and their culture during my time in their country.'\n"," 'I liked this film very much. The story jumps back and forth quite a bit and is not easy to follow. There is no resolution to the story whatsoever, and you are left to wonder what really happened. Since I like that sort of film I enjoyed this. I especially like the \"dating\" scenes between the boys and I was drawn into their lives. And of course any film with a naked Staphane Rideau will get a couple of extra points. ;-)'\n"," 'There\\'s a part of me that would like to give this movie a high rating. Considering that it was made in 1953, this is a very courageous movie about transvestites, tackling the issue fairly seriously and sympathetically (and offering the viewer a lot of information on the subject) and trying very hard not to stereotype. The movie clearly makes the point that transvestites are not homosexuals, and that aside from wearing women\\'s clothing they lead a relatively normal life. It deals with the pain of not being accepted in society - the plot revolves around a police officer (Lyle Talbot) desperately trying to understand the issue because of the recent suicide of a transvestite. So, you have to give everyone involved with this movie credit for taking on such a controversial (in the context of 1953) subject.<br /><br />Having said all that, I\\'m also sorry to say that this movie is absolutely dreadful. In trying to portray Glen/Glenda\\'s (Edward D. Wood) pain, the movie falls into silly (and at times surprisingly - again given the era - sensual) fantasies that make the story very hard to follow. The acting is wooden at best. None of the dialogue comes across as real; the actors look and sound like people reading speeches written by others. And - worst of all - there was no point to having Bela Lugosi in this movie. This was another of the increasingly embarrassing roles this poor man took on in the latter stages of his career. \"Pull the strings; pull the strings,\" poor Lugosi\\'s character (called The Spirit in the credits, but really coming across as more of a mad scientist) kept crying. And nothing he did really seemed to have much connection with the rest of the movie.<br /><br />For artistic merit, the movie doesn\\'t really deserve much more than 1/10. However, for the courage involved in just putting it out, I\\'ll give it a 3/10.'\n"," ...\n"," 'I don\\'t even know where to begin on this one. \"It\\'s all about the family.\" That has to be the worst line of dialogue ever heard in a \"horror\" movie, although this couldn\\'t be a horror movie even if it tried!!! Ugh!!! And I know that Owen Wilson is a better actor. He needs to stop playing the token guy who dies in every action movie (Anaconda, Armageddon). After all, the man did co-write \"Bottle Rocket\" and \"Rushmore.\" He does have some talent. Also, Lily Taylor should stick to indie films. She has no place here. Finally, Catherine Zeta-Jones should become a porn star. There\\'s no room in legitimate acting for her. I\\'m serious. One of the worst movies I\\'ve ever seen, EVER.'\n"," \"Richard Tyler is a little boy who is scared of everything. He doesn't like riding his bike or climbing on his tree house because he knows what kind of accidents might happen to him. So one day he is riding his bike and because it is starting to rain, he decides to wait in the library until it stops raining. In there the whole story takes place. He experiences all kinds of staff and in the end he is not scared any more. But the whole story is unbelievable and even good actors like Macaulay Culkin could not make the story better than it is.\"\n"," 'I waited long to watch this movie. Also because I like Bruce Willis. The plot was quite different from what I had expected but still quite good. Its a good mix of emotions, humor and drama.<br /><br />Left me thinking over and again :)']\n","\n","y_test:\n"," [1 1 0 ... 0 0 1]\n"]}]},{"cell_type":"markdown","metadata":{"id":"sVpWSvr64-RV"},"source":["Now create a function that cleans and tokenizes the texts and try it out.\n","- `tokenize_text` removes unwanted characters, transforms the text to lowercase...\n","- It also returns a list of words (it tokenizes the original string)."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":521},"id":"MtpOprrN4-RW","executionInfo":{"status":"ok","timestamp":1638025816136,"user_tz":-60,"elapsed":1753,"user":{"displayName":"Riccardo Lunardi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOLqvlm8PcUAGFVPWbdrl6nABLjES31yTkFo78zQ=s64","userId":"12776781988421852239"}},"outputId":"dfdb2ea7-bca4-4c16-be95-8d57bce99c42"},"source":["import string\n","import nltk\n","nltk.download('punkt')\n","from nltk import word_tokenize\n","\n","def tokenize_text(text):\n","\n","    text = text.replace(\"`\", \"'\")\n","    # separate punctuation from words\n","    for k in string.punctuation:\n","        if k != \"'\":\n","            text = text.replace(k, \" \"+k+\" \")\n","    \n","    text_tokens = word_tokenize(text.lower())\n","    text_clean = text_tokens\n","    return text_clean\n","\n","# Try out function that processes texts\n","print(\"Original text\")\n","text = \"This is a sample text, with some UPPER and lower case words... and punctuation! It's cool.\"\n","display(text)\n","print()\n","print(\"Text after tokenization:\")\n","display(tokenize_text(text))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","Original text\n"]},{"output_type":"display_data","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"This is a sample text, with some UPPER and lower case words... and punctuation! It's cool.\""]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Text after tokenization:\n"]},{"output_type":"display_data","data":{"text/plain":["['this',\n"," 'is',\n"," 'a',\n"," 'sample',\n"," 'text',\n"," ',',\n"," 'with',\n"," 'some',\n"," 'upper',\n"," 'and',\n"," 'lower',\n"," 'case',\n"," 'words',\n"," '.',\n"," '.',\n"," '.',\n"," 'and',\n"," 'punctuation',\n"," '!',\n"," 'it',\n"," \"'s\",\n"," 'cool',\n"," '.']"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"HKw2QiL-gLTN"},"source":["Now that we saw that the function works, let's clean and tokenize all train/test samples."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":136,"referenced_widgets":["1bb85751a025400ca962516dfd90bbbb","697cfec1a51c43a18b821f6d60853adc","f86cdc037ebd46408329e677c6df4033","e39d3ee2025b4cda81ebc9d66ac9ae71","219e62c9379244aa9e90ffd7bf6f930f","ce654102bb1044ccbea197694899a359","2d21b532f3394c39aa137cc24e396479","e9dc51a6df6e452598c893daa18cb6c5","3396de412b4b4c3ab584c9ab7db508f5","671edb6da7b4476a860822ed2cda1aa0","0b31638d3380416ab6809eac11202b0b","acb252a5e2dd445e9c37e4514b7a73a2","1ae7dcd628644f0ba24115c9a0a676f9","2e1d3276283147c383a27c7bd73077b0","c06f37b0d7e2427e820a2e9592592340","5c052faa436d4e37a14719cf84e8138b","cd7b636fb9c74b449fff7a14128e9842","aaec0f72d6de4de4a74a736e1b650def","c1e9ab26fae3427d81c2b361a55e62c8","a24213866b2c4bbb9d12a9cd54b56f19","058bf568db4e47eea0fe10548a660af3","4e30f51b5c824df4b1a98ddd271dcc43"]},"id":"LCNypr_84-RZ","executionInfo":{"status":"ok","timestamp":1638025947463,"user_tz":-60,"elapsed":131337,"user":{"displayName":"Riccardo Lunardi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOLqvlm8PcUAGFVPWbdrl6nABLjES31yTkFo78zQ=s64","userId":"12776781988421852239"}},"outputId":"b965f921-44ef-4a44-993c-e972947f8db5"},"source":["from tqdm.auto import tqdm\n","\n","tok_train = []\n","\n","for text in tqdm(X_train):\n","    tokenized = tokenize_text(text)  # BLANK\n","    tok_train.append(tokenized)  # BLANK\n","\n","tok_test = []\n","\n","for text in tqdm(X_test): \n","    tokenized = tokenize_text(text)  # BLANK\n","    tok_test.append(tokenized)  # BLANK\n","\n","print()\n","print(tok_train[0])"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1bb85751a025400ca962516dfd90bbbb","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/24998 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"acb252a5e2dd445e9c37e4514b7a73a2","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/25002 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","['in', '1974', ',', 'the', 'teenager', 'martha', 'moxley', '(', 'maggie', 'grace', ')', 'moves', 'to', 'the', 'high', '-', 'class', 'area', 'of', 'belle', 'haven', ',', 'greenwich', ',', 'connecticut', '.', 'on', 'the', 'mischief', 'night', ',', 'eve', 'of', 'halloween', ',', 'she', 'was', 'murdered', 'in', 'the', 'backyard', 'of', 'her', 'house', 'and', 'her', 'murder', 'remained', 'unsolved', '.', 'twenty', '-', 'two', 'years', 'later', ',', 'the', 'writer', 'mark', 'fuhrman', '(', 'christopher', 'meloni', ')', ',', 'who', 'is', 'a', 'former', 'la', 'detective', 'that', 'has', 'fallen', 'in', 'disgrace', 'for', 'perjury', 'in', 'o', '.', 'j', '.', 'simpson', 'trial', 'and', 'moved', 'to', 'idaho', ',', 'decides', 'to', 'investigate', 'the', 'case', 'with', 'his', 'partner', 'stephen', 'weeks', '(', 'andrew', 'mitchell', ')', 'with', 'the', 'purpose', 'of', 'writing', 'a', 'book', '.', 'the', 'locals', 'squirm', 'and', 'do', 'not', 'welcome', 'them', ',', 'but', 'with', 'the', 'support', 'of', 'the', 'retired', 'detective', 'steve', 'carroll', '(', 'robert', 'forster', ')', 'that', 'was', 'in', 'charge', 'of', 'the', 'investigation', 'in', 'the', '70', \"'s\", ',', 'they', 'discover', 'the', 'criminal', 'and', 'a', 'net', 'of', 'power', 'and', 'money', 'to', 'cover', 'the', 'murder', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', '``', 'murder', 'in', 'greenwich', '``', 'is', 'a', 'good', 'tv', 'movie', ',', 'with', 'the', 'true', 'story', 'of', 'a', 'murder', 'of', 'a', 'fifteen', 'years', 'old', 'girl', 'that', 'was', 'committed', 'by', 'a', 'wealthy', 'teenager', 'whose', 'mother', 'was', 'a', 'kennedy', '.', 'the', 'powerful', 'and', 'rich', 'family', 'used', 'their', 'influence', 'to', 'cover', 'the', 'murder', 'for', 'more', 'than', 'twenty', 'years', '.', 'however', ',', 'a', 'snoopy', 'detective', 'and', 'convicted', 'perjurer', 'in', 'disgrace', 'was', 'able', 'to', 'disclose', 'how', 'the', 'hideous', 'crime', 'was', 'committed', '.', 'the', 'screenplay', 'shows', 'the', 'investigation', 'of', 'mark', 'and', 'the', 'last', 'days', 'of', 'martha', 'in', 'parallel', ',', 'but', 'there', 'is', 'a', 'lack', 'of', 'the', 'emotion', 'in', 'the', 'dramatization', '.', 'my', 'vote', 'is', 'seven', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'title', '(', 'brazil', ')', ':', 'not', 'available']\n"]}]},{"cell_type":"markdown","metadata":{"id":"XOwW0SD64-Rc"},"source":["<a name=\"2\"></a>\n","# 2)  Building the Vocabulary\n","\n","Now let's build the Vocabulary.\n","- Map each word in each text to an integer (an \"index\"). \n","- Note that you will build the vocabulary based on the **training data**. \n","- To do so, you will assign an index to every word by iterating over your training set.\n","\n","The vocabulary will also include some special tokens\n","- `__PAD__`: padding, which we will use to make all sentences of the same length\n","- `__UNK__`: a token representing any word that is not in the vocabulary."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bSXzQh684-Rc","executionInfo":{"status":"ok","timestamp":1638025949103,"user_tz":-60,"elapsed":1651,"user":{"displayName":"Riccardo Lunardi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOLqvlm8PcUAGFVPWbdrl6nABLjES31yTkFo78zQ=s64","userId":"12776781988421852239"}},"outputId":"fa601d0c-f48f-4173-8336-3f94deec6a37"},"source":["# Include special tokens \n","# started with pad and unk tokens\n","Vocab = {'__PAD__': 0, '__UNK__': 1} \n","\n","# Note that we build vocab using training data\n","for tok_text in tok_train:\n","    for word in tok_text:  # BLANK\n","        if word not in Vocab:  # BLANK \n","            Vocab[word] = len(Vocab)\n","    \n","print(\"Total words in vocab are\", len(Vocab))\n","# print first 6 elements of Vocab\n","print(\"{\")\n","for k,v in list(Vocab.items())[:6]:\n","    print(f\"  '{k}': {v},\")\n","print(\"  ...\\n}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total words in vocab are 80727\n","{\n","  '__PAD__': 0,\n","  '__UNK__': 1,\n","  'in': 2,\n","  '1974': 3,\n","  ',': 4,\n","  'the': 5,\n","  ...\n","}\n"]}]},{"cell_type":"markdown","metadata":{"id":"1nYyaCnn4-Rf"},"source":["The dictionary `Vocab` will look like this:\n","```CPP\n","{'__PAD__': 0, \n"," '__UNK__': 1,\n"," 'in': 2,\n"," '1974': 3,\n"," ',': 4,\n"," 'the': 5,\n"," ...\n","```\n","\n","- Each unique word has a unique integer associated with it.\n","- The total number of words in Vocab: 80727"]},{"cell_type":"markdown","metadata":{"id":"zA8CWeLJ4-Rg"},"source":["<a name=\"3\"></a>\n","# 3)  Numericalization and Padding\n","\n","Write a function that will convert each text to a list of text ids (a list of unique integer IDs representing the processed text).\n","For words in the text that are not in the vocabulary, set them to the unique ID for the token `__UNK__`.\n","\n","**Example**\n","\n","Input a text:\n","```\n","'Let the hypotenuse of the triangle be x'\n","```\n","\n","The tweet_to_tensor will first conver the tweet into a list of tokens (including only relevant words)\n","```\n","['let', 'the', 'hypotenuse', 'of', 'the', 'triangle', 'be', 'x']\n","```\n","\n","Then it will convert each word into its unique integer\n","\n","```\n","[807, 5, 0, 19, 5, 17099, 288, 5769]\n","```\n","- Notice that the word \"hypotenuse\" is not in the vocabulary, so it is assigned the unique integer associated with the `__UNK__` token, because it is considered \"unknown.\"\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VJF2LUyM30Pt","executionInfo":{"status":"ok","timestamp":1638025949104,"user_tz":-60,"elapsed":23,"user":{"displayName":"Riccardo Lunardi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOLqvlm8PcUAGFVPWbdrl6nABLjES31yTkFo78zQ=s64","userId":"12776781988421852239"}},"outputId":"92329d57-7aca-47bc-ec56-8ffbc94f6266"},"source":["text = \"Let the hypotenuse of the triangle be x\"\n","toks = tokenize_text(text)\n","for tok in toks:\n","    print(Vocab.get(tok), \"\\t\", tok)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["807 \t let\n","5 \t the\n","None \t hypotenuse\n","19 \t of\n","5 \t the\n","17099 \t triangle\n","288 \t be\n","5769 \t x\n"]}]},{"cell_type":"markdown","metadata":{"id":"AOiVnKSp4-Rg"},"source":["Let's write a function `text_to_ids` that takes in a text and converts it to an array of numbers. Use the `Vocab` dictionary you have just created to numericalize the texts. \\\\\n","Use the `vocab_dict` parameter and not a global variable.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sSdoOCE-4-Rh","executionInfo":{"status":"ok","timestamp":1638025949105,"user_tz":-60,"elapsed":19,"user":{"displayName":"Riccardo Lunardi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOLqvlm8PcUAGFVPWbdrl6nABLjES31yTkFo78zQ=s64","userId":"12776781988421852239"}},"outputId":"5869c73a-5c77-40ce-f1ae-266c2f6f5131"},"source":["def text_to_ids(tokenized_text, vocab_dict, unk_token='__UNK__'):\n","        \n","    # Initialize the list that will contain the unique integer IDs of each word\n","    text_id_list = []\n","    \n","    # Get the unique integer ID of the __UNK__ token using the vocab_dict\n","    unk_ID = vocab_dict[unk_token]  # BLANK\n","        \n","    # for each word in the list:\n","    for word in tokenized_text:\n","        \n","        # Get the unique integer ID.\n","        # If the word doesn't exist in the vocab dictionary,\n","        # use the unique ID for __UNK__ instead.\n","        word_ID = vocab_dict[word] if vocab_dict.get(word) else unk_ID # BLANK\n","        \n","        # Append the unique integer ID to the text_id_list.\n","        text_id_list.append(word_ID)  # BLANK\n","    \n","    return text_id_list\n","\n","print(\"Actual text is\\n\", X_test[1])\n","print(\"Processed text is\\n\", tok_test[1])\n","print(\"\\nText_ids of text:\\n\", text_to_ids(tok_test[1], vocab_dict=Vocab))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Actual text is\n"," I liked this film very much. The story jumps back and forth quite a bit and is not easy to follow. There is no resolution to the story whatsoever, and you are left to wonder what really happened. Since I like that sort of film I enjoyed this. I especially like the \"dating\" scenes between the boys and I was drawn into their lives. And of course any film with a naked Staphane Rideau will get a couple of extra points. ;-)\n","Processed text is\n"," ['i', 'liked', 'this', 'film', 'very', 'much', '.', 'the', 'story', 'jumps', 'back', 'and', 'forth', 'quite', 'a', 'bit', 'and', 'is', 'not', 'easy', 'to', 'follow', '.', 'there', 'is', 'no', 'resolution', 'to', 'the', 'story', 'whatsoever', ',', 'and', 'you', 'are', 'left', 'to', 'wonder', 'what', 'really', 'happened', '.', 'since', 'i', 'like', 'that', 'sort', 'of', 'film', 'i', 'enjoyed', 'this', '.', 'i', 'especially', 'like', 'the', '``', 'dating', '``', 'scenes', 'between', 'the', 'boys', 'and', 'i', 'was', 'drawn', 'into', 'their', 'lives', '.', 'and', 'of', 'course', 'any', 'film', 'with', 'a', 'naked', 'staphane', 'rideau', 'will', 'get', 'a', 'couple', 'of', 'extra', 'points', '.', ';', '-', ')']\n","\n","Text_ids of text:\n"," [158, 418, 186, 472, 377, 403, 24, 5, 113, 6964, 254, 36, 5213, 369, 51, 474, 36, 50, 83, 164, 14, 1624, 24, 145, 50, 506, 6490, 14, 5, 113, 966, 4, 36, 277, 295, 2106, 14, 1106, 219, 159, 387, 24, 871, 158, 160, 55, 1988, 19, 472, 158, 470, 186, 24, 158, 357, 160, 5, 108, 6593, 108, 1117, 576, 5, 990, 36, 158, 31, 2940, 739, 127, 483, 24, 36, 19, 656, 292, 472, 70, 51, 4073, 1, 19879, 178, 764, 51, 1273, 19, 6882, 2934, 24, 840, 16, 12]\n"]}]},{"cell_type":"markdown","metadata":{"id":"GRASwTU0BfwN"},"source":["**Expected output**\n","\n","```\n","Actual text is\n"," I liked this film very much. The story jumps back and forth quite a bit and is not easy to follow. There is no resolution to the story whatsoever, and you are left to wonder what really happened. Since I like that sort of film I enjoyed this. I especially like the \"dating\" scenes between the boys and I was drawn into their lives. And of course any film with a naked Staphane Rideau will get a couple of extra points. ;-)\n","Processed text is\n"," ['i', 'liked', 'this', 'film', 'very', 'much', '.', 'the', 'story', 'jumps', 'back', 'and', 'forth', 'quite', 'a', 'bit', 'and', 'is', 'not', 'easy', 'to', 'follow', '.', 'there', 'is', 'no', 'resolution', 'to', 'the', 'story', 'whatsoever', ',', 'and', 'you', 'are', 'left', 'to', 'wonder', 'what', 'really', 'happened', '.', 'since', 'i', 'like', 'that', 'sort', 'of', 'film', 'i', 'enjoyed', 'this', '.', 'i', 'especially', 'like', 'the', '``', 'dating', '``', 'scenes', 'between', 'the', 'boys', 'and', 'i', 'was', 'drawn', 'into', 'their', 'lives', '.', 'and', 'of', 'course', 'any', 'film', 'with', 'a', 'naked', 'staphane', 'rideau', 'will', 'get', 'a', 'couple', 'of', 'extra', 'points', '.', ';', '-', ')']\n","\n","Text_ids of text:\n"," [158, 418, 186, 472, 377, 403, 24, 5, 113, 6964, 254, 36, 5213, 369, 51, 474, 36, 50, 83, 164, 14, 1624, 24, 145, 50, 506, 6490, 14, 5, 113, 966, 4, 36, 277, 295, 2106, 14, 1106, 219, 159, 387, 24, 871, 158, 160, 55, 1988, 19, 472, 158, 470, 186, 24, 158, 357, 160, 5, 108, 6593, 108, 1117, 576, 5, 990, 36, 158, 31, 2940, 739, 127, 483, 24, 36, 19, 656, 292, 472, 70, 51, 4073, 1, 19879, 178, 764, 51, 1273, 19, 6882, 2934, 24, 840, 16, 12]\n"," ```"]},{"cell_type":"markdown","metadata":{"id":"ntn1_mKr4-Rp"},"source":["All samples which will be fed to our simple neural network should be truncated or padded to the same length. \n","Let's set a `MAX_SEQ_LEN` of 320 tokens and pad (lengthen) or truncate (cut) all sequences to this lenght.\n","\n","Padding is done by using the `__PAD__` token in `Vocab`."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"djx-nWWB4-Rt","executionInfo":{"status":"ok","timestamp":1638025949105,"user_tz":-60,"elapsed":16,"user":{"displayName":"Riccardo Lunardi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOLqvlm8PcUAGFVPWbdrl6nABLjES31yTkFo78zQ=s64","userId":"12776781988421852239"}},"outputId":"1e2bf1cc-c723-446d-8245-0c093afa9590"},"source":["MAX_SEQ_LEN = 320\n","\n","def pad_sequence(ids, max_len, pad_id):\n","    \n","    n_pad = max_len - len(ids)   # BLANK\n","    \n","    # If the sequence is too short: pad it\n","    if n_pad >= 0: \n","        # Generate a list of pad_id, with length n_pad\n","        padding = n_pad*[pad_id]  # BLANK\n","        # concatenate the tensor and the list of padded zeros\n","        out = ids + padding  # BLANK\n","    \n","    # If the sequence is too long: cut it\n","    elif n_pad < 0: \n","        out = ids[:max_len]  # BLANK\n","    \n","    return out\n","\n","example_sentence_ids = [1,2,3,4,5,6,7,8,9]\n","print(\"This is an examples of sentence ids (length 9):\")\n","print(example_sentence_ids, \"\\n\")\n","\n","print(\"This is the same sequence padded to 13:\")\n","print(\"|<----- it should be this long ------>|\")\n","print(pad_sequence(example_sentence_ids, 13, 0), \"\\n\")\n","\n","print(\"This is the same sequence padded to 5:\")\n","print(\"|<----------->|\")\n","print(pad_sequence(example_sentence_ids, 5, 0))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["This is an examples of sentence ids (length 9):\n","[1, 2, 3, 4, 5, 6, 7, 8, 9] \n","\n","This is the same sequence padded to 13:\n","|<----- it should be this long ------>|\n","[1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 0, 0, 0] \n","\n","This is the same sequence padded to 5:\n","|<----------->|\n","[1, 2, 3, 4, 5]\n"]}]},{"cell_type":"markdown","metadata":{"id":"kkq926OwlnCW"},"source":["If the test above turned out right, we can numericalize and pad all the samples of the two datasets.\n","This will turn our data (strings of various lengths) into inputs which are understandable for the neural network (**equally sized sequences of integers**)."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YgtNFzap4-Rv","executionInfo":{"status":"ok","timestamp":1638025951707,"user_tz":-60,"elapsed":2614,"user":{"displayName":"Riccardo Lunardi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOLqvlm8PcUAGFVPWbdrl6nABLjES31yTkFo78zQ=s64","userId":"12776781988421852239"}},"outputId":"25e5e15f-45a5-4846-8c55-19ebbbe9ba72"},"source":["ids_train = [text_to_ids(text, Vocab) for text in tok_train]\n","pad_ids_train = [pad_sequence(ids, MAX_SEQ_LEN, Vocab[\"__PAD__\"]) for ids in ids_train]\n","\n","print(len(pad_ids_train))\n","# check if all sequences have the same length MAX_SEQ_LEN\n","print(all([len(seq) == MAX_SEQ_LEN for seq in pad_ids_train]))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["24998\n","True\n"]}]},{"cell_type":"markdown","metadata":{"id":"uCSQB-nC4-Ry"},"source":["Let's do the same for the test samples now"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uTf4iiqQ4-Ry","executionInfo":{"status":"ok","timestamp":1638025954841,"user_tz":-60,"elapsed":3144,"user":{"displayName":"Riccardo Lunardi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOLqvlm8PcUAGFVPWbdrl6nABLjES31yTkFo78zQ=s64","userId":"12776781988421852239"}},"outputId":"0a788d0a-2b76-47c9-eaeb-c2ece09331c8"},"source":["ids_test = [text_to_ids(text, Vocab) for text in tok_test]  # BLANK\n","pad_ids_test = [pad_sequence(ids, MAX_SEQ_LEN, Vocab[\"__PAD__\"]) for ids in ids_test]  # BLANK\n","\n","print(len(pad_ids_test))\n","# check if all sequences have the same length MAX_SEQ_LEN\n","print(all([len(seq) == MAX_SEQ_LEN for seq in pad_ids_test]))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["25002\n","True\n"]}]},{"cell_type":"markdown","metadata":{"id":"avCzAAsi4-SA"},"source":["<a name=\"4\"></a>\n","# 4) Preparing the Embeddings\n","\n","Now we have a sequence of integers for each text (`pad_ids_train` and `pad_ids_train`). Integes are not very expressive, but you saw in previous lectures that we can use **word embeddings** to create a richer numerical representation of words. \\\\\n","We will use pretrained word embeddings, in this case GloVe (https://nlp.stanford.edu/projects/glove/).\n","\n","We will use them to prepare a `weights_matrix`, which has a row for each word in the `Vocab`. \\\\\n","The two structures need to be aligned to work together:\n","**the n-th row of `weights_matrix` will contain the embedding of the n-th word in `Vocab`**."]},{"cell_type":"markdown","metadata":{"id":"lHlwEDF2nrL7"},"source":["First of all let's check if the download that we started at the beginning is DONE. \\\\\n","If for any reason the download is not proceeding, paste the following command in an empty cell to restart the download. \\\\\n","**(restart the download only if necessary: downloading from zero will take approx 20 minutes)**\n","\n","```! wget -O glove.6B.300d.txt http://ailab.uniud.it/wp-content/uploads/2019/05/glove.6B.300d.txt```"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3qcL0srMpSYa","executionInfo":{"status":"ok","timestamp":1638026867072,"user_tz":-60,"elapsed":912234,"user":{"displayName":"Riccardo Lunardi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOLqvlm8PcUAGFVPWbdrl6nABLjES31yTkFo78zQ=s64","userId":"12776781988421852239"}},"outputId":"77addcdc-b10f-4c63-bae1-8b36e1cecb38"},"source":["# Wait for the embeddings to finish downloading\n","import time\n","from IPython.display import clear_output\n","\n","# this will show you the output of the previous wget command, while the download in in progress\n","while not os.path.exists(\"DONE\"):\n","    #print(\"Wait for the embeddings to be downloaded. Here is the progress\")\n","    ! tail -n 2 progress.txt | head -n 1\n","    clear_output(wait=True)\n","\n","print(\"Embeddings downloaded successfully!\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Embeddings downloaded successfully!\n"]}]},{"cell_type":"markdown","metadata":{"id":"6nie5Jjj4-SF"},"source":["The embeddings you just downloaded in `glove.6B.300d.txt` are given in the following format:\n","\n","```\n","word_1 <space> f1 <space> f2 <space> ... <space> f299 <space> f300 <\\n>\n","word_2 <space> f1 <space> f2 <space> ... <space> f299 <space> f300 <\\n>\n","...\n","word_N <space> f1 <space> f2 <space> ... <space> f299 <space> f300 <\\n>\n","```\n","\n","For each word we need to recover the vector `[f1, f2, ..., f300]`, which is a 300-dimensional GloVe embedding.\n","\n","We are going to read the file and save:\n","`glove.6B.300d_DICT.pkl`: a word-vector dictionary with an entry for each word in `glove.6B.300d.txt` "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":986,"referenced_widgets":["140b2f9da23d41fb8952f76b34b94e59","69929bd823dc4dd7934c58f179e5d390","22cb2a0c7631410399dbfea808a881da","8b458b530c424480983feed25cebb679","2b1c0d313f904507825a7d011a551168","5a8e78a3e7b143e9b171a86c212cd824","82adcb67c5784587beed7aad71db093e","411802e10dc7404691f6584d163b1d13","16befc156ae04e12a4edac5072c1642b","df79680830a342af9131279454a6760e","8893ddc56c134e4eaa029973de13c9dd"]},"id":"9BMAipfn4-SF","scrolled":true,"executionInfo":{"status":"ok","timestamp":1638026975305,"user_tz":-60,"elapsed":108241,"user":{"displayName":"Riccardo Lunardi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOLqvlm8PcUAGFVPWbdrl6nABLjES31yTkFo78zQ=s64","userId":"12776781988421852239"}},"outputId":"d3fa521e-a2c9-46c3-b71d-dccfec65377e"},"source":["import pickle\n","import numpy as np\n","\n","# This will take a approx 2 minutes\n","\n","if not os.path.exists(\"glove.6B.300d_DICT.pkl\"):\n","    print(\"Extracting embeddings from glove.6B.300d\")\n","    glove = {}\n","    with open(f'glove.6B.300d.txt', 'rb') as f:\n","        lines = f.readlines()\n","        for l in tqdm(lines):\n","            line = l.decode().split()\n","            word = line[0]\n","            vect = np.array(line[1:]).astype(np.float)\n","            glove[word] = vect\n","    \n","    pickle.dump(glove, open(f'glove.6B.300d_DICT.pkl', 'wb'))\n","    print()\n","\n","print(\"Loading embeddings\")\n","glove = pickle.load(open(f'glove.6B.300d_DICT.pkl', 'rb'))\n","\n","print(\"Example: embedding of the word 'the'\")\n","print(\"the\", glove[\"the\"])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Extracting embeddings from glove.6B.300d\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"140b2f9da23d41fb8952f76b34b94e59","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/400000 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Loading embeddings\n","Example: embedding of the word 'the'\n","the [ 4.6560e-02  2.1318e-01 -7.4364e-03 -4.5854e-01 -3.5639e-02  2.3643e-01\n"," -2.8836e-01  2.1521e-01 -1.3486e-01 -1.6413e+00 -2.6091e-01  3.2434e-02\n","  5.6621e-02 -4.3296e-02 -2.1672e-02  2.2476e-01 -7.5129e-02 -6.7018e-02\n"," -1.4247e-01  3.8825e-02 -1.8951e-01  2.9977e-01  3.9305e-01  1.7887e-01\n"," -1.7343e-01 -2.1178e-01  2.3617e-01 -6.3681e-02 -4.2318e-01 -1.1661e-01\n","  9.3754e-02  1.7296e-01 -3.3073e-01  4.9112e-01 -6.8995e-01 -9.2462e-02\n","  2.4742e-01 -1.7991e-01  9.7908e-02  8.3118e-02  1.5299e-01 -2.7276e-01\n"," -3.8934e-02  5.4453e-01  5.3737e-01  2.9105e-01 -7.3514e-03  4.7880e-02\n"," -4.0760e-01 -2.6759e-02  1.7919e-01  1.0977e-02 -1.0963e-01 -2.6395e-01\n","  7.3990e-02  2.6236e-01 -1.5080e-01  3.4623e-01  2.5758e-01  1.1971e-01\n"," -3.7135e-02 -7.1593e-02  4.3898e-01 -4.0764e-02  1.6425e-02 -4.4640e-01\n","  1.7197e-01  4.6246e-02  5.8639e-02  4.1499e-02  5.3948e-01  5.2495e-01\n","  1.1361e-01 -4.8315e-02 -3.6385e-01  1.8704e-01  9.2761e-02 -1.1129e-01\n"," -4.2085e-01  1.3992e-01 -3.9338e-01 -6.7945e-02  1.2188e-01  1.6707e-01\n","  7.5169e-02 -1.5529e-02 -1.9499e-01  1.9638e-01  5.3194e-02  2.5170e-01\n"," -3.4845e-01 -1.0638e-01 -3.4692e-01 -1.9024e-01 -2.0040e-01  1.2154e-01\n"," -2.9208e-01  2.3353e-02 -1.1618e-01 -3.5768e-01  6.2304e-02  3.5884e-01\n","  2.9060e-02  7.3005e-03  4.9482e-03 -1.5048e-01 -1.2313e-01  1.9337e-01\n","  1.2173e-01  4.4503e-01  2.5147e-01  1.0781e-01 -1.7716e-01  3.8691e-02\n","  8.1530e-02  1.4667e-01  6.3666e-02  6.1332e-02 -7.5569e-02 -3.7724e-01\n","  1.5850e-02 -3.0342e-01  2.8374e-01 -4.2013e-02 -4.0715e-02 -1.5269e-01\n","  7.4980e-02  1.5577e-01  1.0433e-01  3.1393e-01  1.9309e-01  1.9429e-01\n","  1.5185e-01 -1.0192e-01 -1.8785e-02  2.0791e-01  1.3366e-01  1.9038e-01\n"," -2.5558e-01  3.0400e-01 -1.8960e-02  2.0147e-01 -4.2110e-01 -7.5156e-03\n"," -2.7977e-01 -1.9314e-01  4.6204e-02  1.9971e-01 -3.0207e-01  2.5735e-01\n","  6.8107e-01 -1.9409e-01  2.3984e-01  2.2493e-01  6.5224e-01 -1.3561e-01\n"," -1.7383e-01 -4.8209e-02 -1.1860e-01  2.1588e-03 -1.9525e-02  1.1948e-01\n","  1.9346e-01 -4.0820e-01 -8.2966e-02  1.6626e-01 -1.0601e-01  3.5861e-01\n","  1.6922e-01  7.2590e-02 -2.4803e-01 -1.0024e-01 -5.2491e-01 -1.7745e-01\n"," -3.6647e-01  2.6180e-01 -1.2077e-02  8.3190e-02 -2.1528e-01  4.1045e-01\n","  2.9136e-01  3.0869e-01  7.8864e-02  3.2207e-01 -4.1023e-02 -1.0970e-01\n"," -9.2041e-02 -1.2339e-01 -1.6416e-01  3.5382e-01 -8.2774e-02  3.3171e-01\n"," -2.4738e-01 -4.8928e-02  1.5746e-01  1.8988e-01 -2.6642e-02  6.3315e-02\n"," -1.0673e-02  3.4089e-01  1.4106e+00  1.3417e-01  2.8191e-01 -2.5940e-01\n","  5.5267e-02 -5.2425e-02 -2.5789e-01  1.9127e-02 -2.2084e-02  3.2113e-01\n","  6.8818e-02  5.1207e-01  1.6478e-01 -2.0194e-01  2.9232e-01  9.8575e-02\n","  1.3145e-02 -1.0652e-01  1.3510e-01 -4.5332e-02  2.0697e-01 -4.8425e-01\n"," -4.4706e-01  3.3305e-03  2.9264e-03 -1.0975e-01 -2.3325e-01  2.2442e-01\n"," -1.0503e-01  1.2339e-01  1.0978e-01  4.8994e-02 -2.5157e-01  4.0319e-01\n","  3.5318e-01  1.8651e-01 -2.3622e-02 -1.2734e-01  1.1475e-01  2.7359e-01\n"," -2.1866e-01  1.5794e-02  8.1754e-01 -2.3792e-02 -8.5469e-01 -1.6203e-01\n","  1.8076e-01  2.8014e-02 -1.4340e-01  1.3139e-03 -9.1735e-02 -8.9704e-02\n","  1.1105e-01 -1.6703e-01  6.8377e-02 -8.7388e-02 -3.9789e-02  1.4184e-02\n","  2.1187e-01  2.8579e-01 -2.8797e-01 -5.8996e-02 -3.2436e-02 -4.7009e-03\n"," -1.7052e-01 -3.4741e-02 -1.1489e-01  7.5093e-02  9.9526e-02  4.8183e-02\n"," -7.3775e-02 -4.1817e-01  4.1268e-03  4.4414e-01 -1.6062e-01  1.4294e-01\n"," -2.2628e+00 -2.7347e-02  8.1311e-01  7.7417e-01 -2.5639e-01 -1.1576e-01\n"," -1.1982e-01 -2.1363e-01  2.8429e-02  2.7261e-01  3.1026e-02  9.6782e-02\n","  6.7769e-03  1.4082e-01 -1.3064e-02 -2.9686e-01 -7.9913e-02  1.9500e-01\n","  3.1549e-02  2.8506e-01 -8.7461e-02  9.0611e-03 -2.0989e-01  5.3913e-02]\n"]}]},{"cell_type":"markdown","metadata":{"id":"-FqopmsD4-SI"},"source":["We can now use the the dictionary `glove` to get the pretrained GloVe embeddings of many words. \\\\\n","Let's create a `weights_matrix` which has a row for each word in the `Vocab` of our dataset and fill it with the pretrained embeddings when possible.\n","\n","**If a word does not appear in the `glove` dictionary** we will initialize its embedding using a **random vector** drawn from a normal distribution `np.random.normal(scale=0.6, size=(300, ))`. Same goes for the two special tokens `__PAD__` and `__UNK__`."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":101,"referenced_widgets":["616c38f9808849d6bd6762e86035436c","d0353c55743e42d4a2d5489a678e1c45","c6b65bf5aacb4a2ea66e0f38200e5a27","6a72982c75a7468885f4c516267dea76","dde81648134845789341c724844260f3","a418e1604d42403d9df12be4bafeebcb","fbfe9d152d3a4effa38def0b487ba898","77dca599b2df4e31a908d0dae6d0d475","7ebcbe9d631243b99fe77bb90c0d0ca6","926a5150002641bbb28ee235ed17469b","35a02300873049a8b13906d9df6b1f22"]},"id":"GKTiOUDB4-SJ","executionInfo":{"status":"ok","timestamp":1638026976587,"user_tz":-60,"elapsed":1287,"user":{"displayName":"Riccardo Lunardi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOLqvlm8PcUAGFVPWbdrl6nABLjES31yTkFo78zQ=s64","userId":"12776781988421852239"}},"outputId":"6b673975-1356-4b02-a619-c2ab0e08bb0f"},"source":["import torch\n","import random\n","\n","torch.manual_seed(123)\n","torch.cuda.manual_seed(123)\n","np.random.seed(123)\n","random.seed(123)\n","\n","matrix_len = len(Vocab)\n","weights_matrix = np.zeros((matrix_len, 300))\n","\n","# Initialize embeddings for the special tokens __PAD__ and __UNK__\n","weights_matrix[0] = np.random.normal(scale=0.6, size=(300, ))\n","weights_matrix[1] = np.random.normal(scale=0.6, size=(300, ))  # BLANK\n","\n","cnt_words_found = 0\n","cnt_oov = 0\n","\n","words_not_found = []\n","\n","for word, idx in tqdm(Vocab.items()):\n","    # skip special tokens: they were already initialized\n","    if word in [\"__PAD__\",\"__UNK__\"]: \n","        continue\n","    \n","    try: \n","        weights_matrix[idx] = glove[word]  # BLANK\n","        cnt_words_found += 1\n","    except KeyError:\n","        cnt_oov+=1\n","        words_not_found.append(word)\n","        weights_matrix[idx] = np.random.normal(scale=0.6, size=(300, ))  # BLANK\n","\n","print(\"out of vocab words: \", cnt_oov)\n","print(\"words found:        \", cnt_words_found)\n","print(\"total words:        \", cnt_oov + cnt_words_found)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"616c38f9808849d6bd6762e86035436c","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/80727 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["out of vocab words:  18486\n","words found:         62239\n","total words:         80725\n"]}]},{"cell_type":"markdown","metadata":{"id":"dC4w7gfV4-SO"},"source":["**Expected output**\n","```\n","out of vocab words:  18486\n","words found:         62239\n","total words:         80725\n","```"]},{"cell_type":"markdown","metadata":{"id":"t9yNoHRLpeIK"},"source":["Let's take a look at the contents of the `weights_matrix`:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":277},"id":"lhgzxb7L4-SO","executionInfo":{"status":"ok","timestamp":1638026976788,"user_tz":-60,"elapsed":17,"user":{"displayName":"Riccardo Lunardi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOLqvlm8PcUAGFVPWbdrl6nABLjES31yTkFo78zQ=s64","userId":"12776781988421852239"}},"outputId":"a6e6990d-ee1e-48e3-be47-6dc504a52af2"},"source":["display(weights_matrix)\n","print(weights_matrix.shape)\n","print(len(Vocab))"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["array([[-0.65137836,  0.59840727,  0.1697871 , ...,  0.24941672,\n","         0.09632665,  0.49185637],\n","       [ 0.45903291, -0.4973933 , -0.39549079, ..., -0.83662507,\n","         0.54761276, -0.76414214],\n","       [-0.44399   ,  0.12817   , -0.25247   , ..., -0.20043   ,\n","        -0.082191  , -0.06255   ],\n","       ...,\n","       [ 0.45443   , -0.19099   ,  0.012089  , ...,  0.77892   ,\n","         0.35793   , -0.2189    ],\n","       [ 0.25495337, -0.14032335,  0.13888993, ...,  0.56438958,\n","         0.89429585,  1.09692969],\n","       [-0.81127196,  0.72867943, -1.13618151, ...,  0.95413854,\n","         1.79100622,  0.85883073]])"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["(80727, 300)\n","80727\n"]}]},{"cell_type":"markdown","metadata":{"id":"vDauc1BCCtzD"},"source":["**Expected output** (the first two rows might be different because they are initialized randomly).\n","\n","```\n","array([[-0.65137836,  0.59840727,  0.1697871 , ...,  0.24941672,\n","         0.09632665,  0.49185637],\n","       [ 0.45903291, -0.4973933 , -0.39549079, ..., -0.83662507,\n","         0.54761276, -0.76414214],\n","       [-0.44399   ,  0.12817   , -0.25247   , ..., -0.20043   ,\n","        -0.082191  , -0.06255   ],\n","       ...,\n","       [ 0.45443   , -0.19099   ,  0.012089  , ...,  0.77892   ,\n","         0.35793   , -0.2189    ],\n","       [ 0.25495337, -0.14032335,  0.13888993, ...,  0.56438958,\n","         0.89429585,  1.09692969],\n","       [-0.81127196,  0.72867943, -1.13618151, ...,  0.95413854,\n","         1.79100622,  0.85883073]])\n","(80727, 300)\n","80727\n","```"]},{"cell_type":"markdown","metadata":{"id":"mm7vAJMd4-SR"},"source":["Now the `weights_matrix` is ready to be used later in our final model: it will translate any `text_id` to a 300-dimensional embedding."]},{"cell_type":"markdown","metadata":{"id":"81OXCeiV4-Sj"},"source":["<a name=\"5\"></a>\n","# 5) Setup the Model\n","\n","Now you will implement a classifier using neural networks.\n","\n","For the model implementation, you will use some pytorch layers from `torch.nn` (`nn.Embedding`, `nn.Linear`, `nn.LogSoftmax`). To know more about their inputs/outputs type a command like `help(torch.nn.Embeddings)` in an emptycell or look for the official documentation online. \\\\\n","This is a more detailed schema of the model we will create:\n","\n","\\\\\n","$$\n","\\small{\\textrm{input ids}}: \n","\\small{(\\textit{batch_size} \\times \\textit{max_seq_len})}\\\\\n","\\boxed{\\ \\small{\\textbf{Embedding Layer}\\\\ \\texttt{nn.Embedding}}\\ } \\\\\n","%\n","\\small{\\textrm{word embeddings}}:\n","\\small{(\\textit{batch_size} \\times \\textit{embedding_size} \\times \\textit{max_seq_len})}\\\\\n","\\boxed{\\ \\small{\\textbf{Mean}\\\\ \\texttt{torch.mean}}\\ } \\\\\n","%\n","\\small{\\textrm{sentence embeddings}}:\n","\\small{(\\textit{batch_size} \\times \\textit{embedding_size})}\\\\\n","\\boxed{\\ \\small{\\textbf{Linear Layer}\\\\ \\texttt{nn.Linear}}\\ } \\\\\n","%\n","\\small{\\textrm{raw outputs}}:\n","\\small{(\\textit{batch_size} \\times \\textit{2})}\\\\\n","\\boxed{\\ \\small{\\textbf{Soft Max}\\\\ \\texttt{nn.SoftMax}}\\ } \\\\\n","%\n","\\small{\\textrm{output probabilities / logits}}:\n","\\small{(\\textit{batch_size} \\times \\textit{2})}\\\\\n","$$\n","\n","It will:\n","- take a sequence of word input ids\n","- get the embedding of each word using an `nn.Embedding` layer initialized with GloVe embeddings\n","- calculate the mean embedding of the input using `torch.mean`\n","- pass the mean embedding to a `nn.Linear` layer which will return a tensor of two values (one for each possible output label)\n","- use a `nn.LogSoftmax` layer to normalize the raw values output by the linear layer into probabilities"]},{"cell_type":"markdown","metadata":{"id":"F9GpTlEW4-Sm"},"source":["About `torch.mean`: you need to specify the **axis** on which to perform the operation. In this case, please choose axis = 1 to get an average embedding vector (an embedding vector that is an average of all words in the sentence). Here is a demo."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":191},"id":"mpLVgWdD4-Sm","executionInfo":{"status":"ok","timestamp":1638026976789,"user_tz":-60,"elapsed":16,"user":{"displayName":"Riccardo Lunardi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOLqvlm8PcUAGFVPWbdrl6nABLjES31yTkFo78zQ=s64","userId":"12776781988421852239"}},"outputId":"15b597fa-97eb-4ab8-e100-27d143f9d61c"},"source":["import numpy as np\n","\n","# Pretend that the embeddings use 3 values to embed the meaning of a word\n","# the sentence has a lenght of 2\n","# and we have a batch size of 4\n","# So the input has a shape (4,2,3)\n","\n","my_inputs = np.array(\n","    [\n","     [ [1,2,3], [4,5,6] ],\n","     [ [1,2,3], [4,5,6] ],\n","     [ [1,2,3], [4,5,6] ],\n","     [ [1,2,3], [4,5,6] ],\n","    ])\n","\n","print(\"The mean along axis 1 creates vectors whose length equals the number of elements in a word embedding\")\n","display(np.mean(my_inputs,axis=1))\n","\n","print(\"The mean along axis 2 creates vectors whose length equals the length of the sentence\")\n","display(np.mean(my_inputs,axis=2))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The mean along axis 1 creates vectors whose length equals the number of elements in a word embedding\n"]},{"output_type":"display_data","data":{"text/plain":["array([[2.5, 3.5, 4.5],\n","       [2.5, 3.5, 4.5],\n","       [2.5, 3.5, 4.5],\n","       [2.5, 3.5, 4.5]])"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["The mean along axis 2 creates vectors whose length equals the length of the sentence\n"]},{"output_type":"display_data","data":{"text/plain":["array([[2., 5.],\n","       [2., 5.],\n","       [2., 5.],\n","       [2., 5.]])"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"1BiyugZp4-So"},"source":["## Model Definition\n","\n","Let's implement the neural netwok we described above in the class `MyModel`!"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":121},"id":"eYY79ff74-Sp","executionInfo":{"status":"ok","timestamp":1638026977286,"user_tz":-60,"elapsed":506,"user":{"displayName":"Riccardo Lunardi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOLqvlm8PcUAGFVPWbdrl6nABLjES31yTkFo78zQ=s64","userId":"12776781988421852239"}},"outputId":"7c0bb2ee-d22a-4386-e3b9-f66726f3bf4d"},"source":["import torch\n","import torch.nn as nn\n","\n","class MyModel(nn.Module):\n","    def __init__(self, embedding_matrix, output_size=2):\n","        super().__init__()\n","\n","        num_embeddings, embedding_dim = embedding_matrix.shape        \n","        self.embedding = nn.Embedding(num_embeddings, embedding_dim)  # BLANK\n","        self.embedding.load_state_dict({'weight': torch.tensor(embedding_matrix)})\n","        # We choose not to train the embeddings further, as they are pretrained\n","        # so we set requires_grad to False\n","        self.embedding.weight.requires_grad = False\n","        \n","        self.linear = torch.nn.Linear(embedding_dim, output_size)  # BLANK\n","        self.softmax = nn.LogSoftmax(dim=-1)\n","        \n","    def forward(self, input_ids):\n","\n","        # input_ids -> embeddings -> mean_embedding -> raw_output -> logits\n","        embeddings = self.embedding(input_ids)\n","        mean_embedding = torch.mean(embeddings,axis=1)\n","        raw_output = self.linear(mean_embedding)\n","        logits = self.softmax(raw_output)\n","        \n","        return logits\n","\n","tmp_model = MyModel(weights_matrix)\n","display(tmp_model)\n","next(tmp_model.parameters()).device"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["MyModel(\n","  (embedding): Embedding(80727, 300)\n","  (linear): Linear(in_features=300, out_features=2, bias=True)\n","  (softmax): LogSoftmax(dim=-1)\n",")"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["device(type='cpu')"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"jWcN9reaETIf"},"source":["**Expected output**\n","\n","```\n","MyModel(\n","  (embedding): Embedding(80727, 300)\n","  (linear): Linear(in_features=300, out_features=2, bias=True)\n","  (softmax): LogSoftmax(dim=-1)\n",")\n","device(type='cpu')\n","```"]},{"cell_type":"markdown","metadata":{"id":"tLkt-gau4-Rm"},"source":["## From numerical inputs to TensorDataset\n","\n","Remember that our train/test data right now are saved in the following variables:\n","- `pad_ids_train`, `y_train`\n","- `pad_ids_test`, `y_test`"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SsLuk3Oy5HPp","executionInfo":{"status":"ok","timestamp":1638026977609,"user_tz":-60,"elapsed":329,"user":{"displayName":"Riccardo Lunardi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOLqvlm8PcUAGFVPWbdrl6nABLjES31yTkFo78zQ=s64","userId":"12776781988421852239"}},"outputId":"54a2c59e-ba80-492d-f2e6-7cccebd8fc1b"},"source":["print(\"First elements of pad_ids_train:\")\n","print(pad_ids_train[0:2])\n","print(\"pad_ids_train of type\", type(pad_ids_train))\n","print()\n","print(\"First elements of y_train:\")\n","print(y_train[0:2])\n","print(\"y_train of type\", type(y_train))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["First elements of pad_ids_train:\n","[[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 5, 15, 16, 17, 18, 19, 20, 21, 4, 22, 4, 23, 24, 25, 5, 26, 27, 4, 28, 19, 29, 4, 30, 31, 32, 2, 5, 33, 19, 34, 35, 36, 34, 37, 38, 39, 24, 40, 16, 41, 42, 43, 4, 5, 44, 45, 46, 9, 47, 48, 12, 4, 49, 50, 51, 52, 53, 54, 55, 56, 57, 2, 58, 59, 60, 2, 61, 24, 62, 24, 63, 64, 36, 65, 14, 66, 4, 67, 14, 68, 5, 69, 70, 71, 72, 73, 74, 9, 75, 76, 12, 70, 5, 77, 19, 78, 51, 79, 24, 5, 80, 81, 36, 82, 83, 84, 85, 4, 86, 70, 5, 87, 19, 5, 88, 54, 89, 90, 9, 91, 92, 12, 55, 31, 2, 93, 19, 5, 94, 2, 5, 95, 96, 4, 97, 98, 5, 99, 36, 51, 100, 19, 101, 36, 102, 14, 103, 5, 37, 24, 104, 105, 106, 107, 104, 105, 106, 107, 108, 37, 2, 22, 108, 50, 51, 109, 110, 111, 4, 70, 5, 112, 113, 19, 51, 37, 19, 51, 114, 42, 115, 116, 55, 31, 117, 118, 51, 119, 6, 120, 121, 31, 51, 122, 24, 5, 123, 36, 124, 125, 126, 127, 128, 14, 103, 5, 37, 59, 129, 130, 40, 42, 24, 131, 4, 51, 132, 54, 36, 133, 134, 2, 58, 31, 135, 14, 136, 137, 5, 138, 139, 31, 117, 24, 5, 140, 141, 5, 94, 19, 45, 36, 5, 142, 143, 19, 7, 2, 144, 4, 86, 145, 50, 51, 146, 19, 5, 147, 2, 5, 148, 24, 149, 150, 50, 151, 24, 104, 105, 106, 107, 104, 105, 106, 107, 152, 9, 153, 12, 154, 83, 155, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [156, 24, 24, 24, 157, 24, 24, 24, 158, 159, 160, 161, 162, 36, 71, 163, 164, 165, 166, 19, 167, 2, 71, 168, 24, 169, 56, 170, 171, 70, 71, 172, 173, 174, 175, 176, 36, 177, 178, 179, 51, 180, 181, 24, 86, 4, 182, 50, 71, 183, 24, 184, 185, 4, 186, 31, 51, 187, 111, 188, 104, 105, 106, 107, 104, 105, 106, 107, 158, 189, 190, 191, 14, 5, 192, 36, 36, 5, 193, 4, 194, 161, 4, 59, 195, 127, 196, 14, 197, 198, 199, 186, 200, 4, 201, 113, 188, 202, 19, 203, 204, 198, 36, 161, 205, 206, 207, 208, 203, 209, 36, 177, 31, 210, 165, 211, 5, 212, 213, 214, 215, 216, 217, 14, 171, 36, 218, 171, 219, 203, 31, 220, 221, 188, 104, 105, 106, 107, 104, 105, 106, 107, 158, 82, 207, 222, 55, 223, 25, 186, 111, 31, 224, 225, 19, 226, 59, 5, 227, 4, 228, 229, 230, 231, 24, 24, 24, 158, 232, 233, 174, 234, 168, 55, 235, 51, 236, 59, 237, 238, 188, 186, 235, 202, 4, 239, 4, 240, 4, 241, 4, 242, 19, 243, 24, 24, 24, 51, 244, 245, 19, 109, 246, 4, 247, 36, 248, 188, 104, 105, 106, 107, 104, 105, 106, 107, 158, 249, 186, 250, 19, 251, 59, 51, 252, 4, 36, 158, 253, 149, 102, 254, 188, 158, 253, 149, 255, 256, 254, 158, 257, 25, 186, 258, 259, 245, 19, 149, 260, 188, 82, 207, 261, 186, 111, 4, 228, 245, 262, 263, 19, 264, 265, 260, 266, 267, 211, 51, 268, 269, 203, 96, 270, 228, 271, 272, 217, 5, 69, 55, 50, 273, 5, 274, 188, 275, 276, 4, 277, 278, 279, 276, 59, 5, 280, 188, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n","pad_ids_train of type <class 'list'>\n","\n","First elements of y_train:\n","[1 0]\n","y_train of type <class 'numpy.ndarray'>\n"]}]},{"cell_type":"markdown","metadata":{"id":"m7CyK6B65YGl"},"source":["**Expected output**\n","\n","```\n","First elements of pad_ids_train:\n","[[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 5, 15, ...\n","pad_ids_train of type <class 'list'>\n","\n","First elements of y_train:\n","[1 0]\n","y_train of type <class 'numpy.ndarray'>\n","```\n","\n","As you can see, neither one of them is a pytorch Tensor, so we need to **convert** them in a suitable format.\n","\n","In addition to just converting them to `LongTensors` (tensors of integers), we will use the `TensorDataset` class provided by `pytorch`, which helps iterating collections of tensors.\n","\n","We will:\n","- convert `pad_ids_<set>` (inputs) and `y_<set>` (outputs) to `LongTensors`\n","- create `<set>_dataset`, a `TensorDataset` with two fields: the inputs and the outputs\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":156},"id":"IyIUeZb04-R1","executionInfo":{"status":"ok","timestamp":1638026977611,"user_tz":-60,"elapsed":10,"user":{"displayName":"Riccardo Lunardi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOLqvlm8PcUAGFVPWbdrl6nABLjES31yTkFo78zQ=s64","userId":"12776781988421852239"}},"outputId":"1b6c8866-f2d8-4f06-acbc-ab562e15dbb5"},"source":["import torch\n","from torch.utils.data import TensorDataset\n","\n","tensor_ids_train = torch.LongTensor(pad_ids_train) # BLANK\n","labels_train = torch.LongTensor(y_train)     # BLANK\n","train_dataset = TensorDataset(tensor_ids_train, labels_train)  # BLANK\n","\n","tensor_ids_test = torch.LongTensor(pad_ids_test)  # BLANK\n","labels_test = torch.LongTensor(y_test)  # BLANK\n","test_dataset = TensorDataset(tensor_ids_test, labels_test)  # BLANK\n","\n","display(tensor_ids_train)\n","display(labels_train)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["tensor([[    2,     3,     4,  ...,     0,     0,     0],\n","        [  156,    24,    24,  ...,     0,     0,     0],\n","        [  281,   281,   281,  ...,    36,   108,   390],\n","        ...,\n","        [  186,  1445,  1498,  ...,   312,   109,   312],\n","        [   51,  2484,    55,  ...,  1561,   525, 11415],\n","        [  158,   253,    14,  ...,   105,   106,   107]])"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tensor([1, 0, 0,  ..., 1, 1, 0])"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"ZJV3f0WR9RyV"},"source":["**Expected output**\n","```\n","tensor([[    2,     3,     4,  ...,     0,     0,     0],\n","        [  156,    24,    24,  ...,     0,     0,     0],\n","        [  281,   281,   281,  ...,    36,   108,   390],\n","        ...,\n","        [  186,  1445,  1498,  ...,   312,   109,   312],\n","        [   51,  2484,    55,  ...,  1561,   525, 11415],\n","        [  158,   253,    14,  ...,   105,   106,   107]])\n","tensor([1, 0, 0,  ..., 1, 1, 0])\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"P2mpgn3A4-R3"},"source":["## Creating a batch generator (DataLoader)\n","\n","Most of the time in Natural Language Processing, and AI in general we use **batches** when training our data sets. \n","- If instead of training with batches of examples, you were to train a model with one example at a time, it would take a very long time to train the model. \n","- We will now use the `class torch.utils.data.DataLoader` to get batches of samples and target labels from the `TensorDataset`s we created before.\n","\n","Once you create the generator, you could include it in a for loop\n","\n","```CPP\n","for batch_inputs, batch_targets in dataloader:\n","    ...\n","```\n","\n","You can also get a single batch like this:\n","\n","```CPP\n","batch_inputs, batch_targets = next(dataloader)\n","```\n","The generator returns the next batch each time it's called. \\\\\n","It returns a tuple containing **the same fields we put in the `TensorDataset`**"]},{"cell_type":"markdown","metadata":{"id":"dRYI0T3i4-R4"},"source":["**Training batches will be sampled randomly** using a `RandomSampler`, in order to break posible patterns in the data.\n","\n","**Test batches can be sampled sequentially** with a `SequentialSampler` (not randomized or shuffled), to make it easier to compare the results with the real labels later."]},{"cell_type":"code","metadata":{"id":"-rR2MIzI4-R4"},"source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","# The DataLoader needs to know our batch size for training\n","batch_size = 16\n","\n","# Create the DataLoaders for our train and test sets.\n","# We'll take training samples in random order. \n","train_dataloader = DataLoader(\n","            train_dataset,\n","            sampler = RandomSampler(train_dataset), # Select batches randomly\n","            batch_size = batch_size\n","        )\n","\n","# Create the test_dataloader on the test_dataset\n","# Use a SequentialSampler instead of a RandomSampler to pull out batches sequentially\n","test_dataloader = DataLoader(\n","            test_dataset,               # BLANK\n","            sampler = SequentialSampler(test_dataset),     # BLANK\n","            batch_size = batch_size  # BLANK\n","        )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ebigfIMO4-R-"},"source":["Now that you have your train/test DataLoader, you can just call them and they will return tensors which correspond to your texts in the first column and their corresponding labels in the second column.\n","\n","Now evrything is setup and we are ready for training."]},{"cell_type":"markdown","metadata":{"id":"xZn-cnYI4-Sv"},"source":["<a name=\"6\"></a>\n","# 6)  Training and Testing\n","\n","We will now define the two functions which will:\n","- train the neural network for one epoch on a given train dataset\n","- test the trained model on a given test dataset\n"]},{"cell_type":"markdown","metadata":{"id":"8MlCV3bJ7y-O"},"source":["Let's start with `train_one_epoch`: it takes as inpu the model, its optimizer (which will take care of updating the weights of the neural network) and the data loader for the train dataset.\n","\n","Its objective is to train the model on all the batches contained in the dataloader: for each batch of input data:\n","- the model produces series of predictions\n","- the loss function `nn.NLLLoss` calculates how much the predictions differ from the real labels\n","- the loss is backpropagated through the network, and the weights of the layers are updated\n","\n","At the same time, we also calculate the accuracy of the predictions, and we report loss and accuracy at the end of the epoch."]},{"cell_type":"code","metadata":{"id":"bHSPod1q4-Sv"},"source":["def train_one_epoch(model, optimizer, data_loader):\n","    running_loss = 0.0\n","    running_correct = 0\n","    num_samples = 0\n","    \n","    for batch_idx , batch in enumerate(data_loader):\n","\n","        text, target = batch\n","        num_samples += target.shape[0]\n","        \n","        device = next(model.parameters()).device\n","        text, target = text.to(device), target.to(device)\n","        \n","        # zero all the gradients\n","        optimizer.zero_grad()  # BLANK\n","\n","        # set the model to \"train\" mode\n","        # and get its outputs\n","        model.train()  # BLANK\n","        output = model(text)  # BLANK\n","        \n","        loss_funct = nn.NLLLoss()\n","        loss = loss_funct(output, target)\n","        \n","        running_loss += loss.item()\n","        preds = torch.argmax(output, axis=1)\n","        running_correct += preds.eq(target.data.view_as(preds)).cpu().sum()\n","        \n","        # perform backward pass on the loss\n","        # and update the paramenters using the optimizer\n","        loss.backward()  # BLANK\n","        optimizer.step()  # BLANK\n","        \n","    loss = running_loss/num_samples\n","    accuracy = running_correct.cpu().numpy()/num_samples\n","    \n","    print('   loss {:<6} | acc {:<6}'.format(round(loss,4), round(accuracy,4)))    \n","    return loss, accuracy\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e0dslnBr9Sic"},"source":["The `test` function takes as input arguments the model and the dataloader for the test set and uses the model for inference. This means that the weights don't need any updates: this is why we don't need to pass the optimizer to the function.\n","\n","We also need to be careful to set the model in evalutation mode (`model.eval()`) and we can stop tracking the operations on the tensors.\n","\n","Just like the training function, the test function return the loss and the accuracy on the test set."]},{"cell_type":"code","metadata":{"id":"feK0g4jS7J73"},"source":["def test(model, data_loader):\n","    running_loss = 0.0\n","    running_correct = 0\n","    num_samples = 0\n","\n","    for batch_idx , batch in enumerate(data_loader):\n","\n","        text, target = batch\n","        num_samples += target.shape[0]\n","        \n","        device = next(model.parameters()).device\n","        text, target = text.to(device), target.to(device)\n","        \n","        # set model in \"eval\" mode\n","        model.eval()  # BLANK\n","        with torch.no_grad():\n","            output = model(text)\n","        \n","        loss_funct = nn.NLLLoss()\n","        loss = loss_funct(output, target)  # BLANK\n","        \n","        running_loss += loss.item()\n","        preds = torch.argmax(output, axis=1)\n","        running_correct += preds.eq(target.data.view_as(preds)).cpu().sum()\n","        \n","    loss = running_loss/num_samples\n","    accuracy = running_correct.cpu().numpy()/num_samples\n","\n","    print('   loss {:<6} | acc {:<6}'.format(round(loss,4), round(accuracy,4)))\n","    \n","    return loss, accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kZEi6Etl-LOR"},"source":["Let's finally create a model and its optimizer and run the training.\n","\n","We will train the model for 20 epochs and perform a test every 5 epochs."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eh8pl9UQ4-Sz","executionInfo":{"status":"ok","timestamp":1638027107546,"user_tz":-60,"elapsed":129724,"user":{"displayName":"Riccardo Lunardi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOLqvlm8PcUAGFVPWbdrl6nABLjES31yTkFo78zQ=s64","userId":"12776781988421852239"}},"outputId":"b8efe8f9-8056-4d26-9a5b-d35434248a44"},"source":["model = MyModel(weights_matrix)\n","model.cuda()\n","# The optimizer needs to know which parameters to update\n","# We generate the list of model.parameters s.t. requires_grad = True\n","# This will exclude the pretrained Embeddings we uploaded before\n","optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-2)\n","print(\"training\")\n","for i in range(35):\n","    print(f\"epoch {i+1:<3}\", end=\"\")\n","    train_one_epoch(model, optimizer, train_dataloader)  # BLANK\n","    if (i+1)%5 == 0:\n","        print(\"testing\")\n","        test(model, test_dataloader)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["training\n","epoch 1     loss 0.0384 | acc 0.6615\n","epoch 2     loss 0.0337 | acc 0.7297\n","epoch 3     loss 0.0318 | acc 0.751 \n","epoch 4     loss 0.0307 | acc 0.7636\n","epoch 5     loss 0.0298 | acc 0.7719\n","testing\n","   loss 0.029  | acc 0.7828\n","epoch 6     loss 0.0295 | acc 0.7763\n","epoch 7     loss 0.0293 | acc 0.7779\n","epoch 8     loss 0.0291 | acc 0.7805\n","epoch 9     loss 0.0286 | acc 0.7827\n","epoch 10    loss 0.0284 | acc 0.786 \n","testing\n","   loss 0.028  | acc 0.7958\n","epoch 11    loss 0.028  | acc 0.7897\n","epoch 12    loss 0.0281 | acc 0.7927\n","epoch 13    loss 0.0279 | acc 0.7926\n","epoch 14    loss 0.0278 | acc 0.794 \n","epoch 15    loss 0.0278 | acc 0.7927\n","testing\n","   loss 0.0286 | acc 0.7835\n","epoch 16    loss 0.0277 | acc 0.7941\n","epoch 17    loss 0.0277 | acc 0.7926\n","epoch 18    loss 0.0272 | acc 0.7993\n","epoch 19    loss 0.0274 | acc 0.8008\n","epoch 20    loss 0.0276 | acc 0.7972\n","testing\n","   loss 0.031  | acc 0.7556\n","epoch 21    loss 0.0272 | acc 0.7977\n","epoch 22    loss 0.0275 | acc 0.7956\n","epoch 23    loss 0.0273 | acc 0.7959\n","epoch 24    loss 0.0271 | acc 0.8003\n","epoch 25    loss 0.0273 | acc 0.8007\n","testing\n","   loss 0.0273 | acc 0.7978\n","epoch 26    loss 0.0273 | acc 0.7992\n","epoch 27    loss 0.0271 | acc 0.8015\n","epoch 28    loss 0.0274 | acc 0.7974\n","epoch 29    loss 0.0271 | acc 0.8015\n","epoch 30    loss 0.027  | acc 0.801 \n","testing\n","   loss 0.0262 | acc 0.8188\n","epoch 31    loss 0.027  | acc 0.8027\n","epoch 32    loss 0.0272 | acc 0.8006\n","epoch 33    loss 0.0269 | acc 0.8033\n","epoch 34    loss 0.027  | acc 0.8034\n","epoch 35    loss 0.0272 | acc 0.8015\n","testing\n","   loss 0.0318 | acc 0.7485\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zJruEmWv4-S3","executionInfo":{"status":"ok","timestamp":1638027110485,"user_tz":-60,"elapsed":2947,"user":{"displayName":"Riccardo Lunardi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOLqvlm8PcUAGFVPWbdrl6nABLjES31yTkFo78zQ=s64","userId":"12776781988421852239"}},"outputId":"92cb3308-466b-428c-8f61-5c199fc5ad31"},"source":["print(\"testing\")\n","test_loss, test_acc = test(model, test_dataloader)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["testing\n","   loss 0.0318 | acc 0.7485\n"]}]},{"cell_type":"markdown","metadata":{"id":"CGpK5LNt_QBZ"},"source":["**Expected output** (may differ slightly because of randomness)\n","\n","```\n","testing\n","   loss 0.0261 | acc 0.818\n","```\n","\n"]}]}